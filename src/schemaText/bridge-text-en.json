{
  "emqx_connector_schema_lib": {
    "auto_reconnect": {
      "desc": "Deprecated. Enable automatic reconnect to the database.",
      "label": "Deprecated. Auto Reconnect Database"
    },
    "password": {
      "desc": "Password used for authentication with the external database.",
      "label": "Password"
    },
    "pool_size": {
      "desc": "Size of the connection pool towards the bridge target service.",
      "label": "Connection Pool Size"
    },
    "prepare_statement": {
      "desc": "Key-value list of SQL prepared statements.",
      "label": "SQL Prepared Statements List"
    },
    "ssl": {
      "desc": "SSL connection settings.",
      "label": "Enable SSL"
    },
    "username": {
      "desc": "The username in the external database used for authentication or identification purposes.",
      "label": "Username"
    },
    "database": {
      "desc": "Database name.",
      "label": "Database Name"
    },
    "description": {
      "label": "Description"
    }
  },
  "emqx_ee_bridge_mqtt": {
    "server": {
      "label": "MQTT Broker"
    },
    "proto_ver": {
      "label": "MQTT Version"
    },
    "bridge_mode": {
      "label": "Bridge Mode",
      "desc": "This setting is only for MQTT protocol version older than 5.0, and the remote MQTT broker MUST support this feature. After being enabled, the remote broker will recognize the current connection as a bridge, that loop detection will be more effective and that retained messages will be propagated correctly."
    },
    "clean_start": {
      "label": "Clean start",
      "desc": "Whether to start a clean session when reconnecting a remote broker for ingress bridge."
    },
    "keepalive": {
      "label": "Keep Alive"
    },
    "retry_interval": {
      "label": "Message Retry Interval",
      "desc": "Retry interval for QoS1/QoS2 messages if no ACK is received."
    },
    "clientid_prefix": {
      "label": "ClientID Prefix",
      "desc": "The prefix used when the connector randomly generates a Client ID."
    },
    "max_inflight": {
      "label": "Max Inflight",
      "desc": "The number of Unacked messages that can be simultaneously waited for during message publishing, in order to improve message delivery efficiency and throughput."
    },
    "topic": {
      "label": "Topic",
      "desc": "Message publishing topic, supports using ${field} syntax to extract variables and dynamically concatenate the topic."
    },
    "source_topic": {
      "label": "Topic",
      "desc": "Message subscription topics support the use of + and # wildcards. When EMQX is running in cluster mode or the connector is configured with a connection pool, shared subscriptions must be used to avoid message duplication."
    },
    "qos": {
      "label": "QoS"
    },
    "retain": {
      "label": "Retain"
    },
    "payload": {
      "label": "Payload",
      "desc": "For example: ${'{'}payload{'}'}, ${'{'}clientid{'}'}, ${'{'}topic{'}'} , ${'{'}username{'}'}, etc. Use fields according to the action requirements of your business and forwards the message as it is if it is empty.Supports reading data using ${'{'}field{'}'} syntax."
    }
  },
  "emqx_ee_bridge_http": {
    "url": {
      "label": "URL",
      "desc": "The URL of the HTTP Connector.<br/>Template with variables is allowed in the path, but variables cannot be used in the host or port part.<br/>For example, `http://localhost:9901/${'{'}topic{'}'}` is allowed, but`http://${'{'}host{'}'}:9901/message` or `http://localhost:${'{'}port{'}'}/message`is not allowed."
    },
    "headers": {
      "label": "Headers"
    },
    "enable_pipelining": {
      "label": "HTTP Pipelining"
    },
    "pool_size": {
      "label": "Connection Pool Size"
    },
    "pool_type": {
      "label": "Pool Type"
    },
    "connect_timeout": {
      "label": "Connect Timeout"
    },
    "body": {
      "label": "Body",
      "desc": "For example: ${'{'}payload{'}'}, ${'{'}clientid{'}'}, ${'{'}topic{'}'} , ${'{'}username{'}'}, etc. Use fields according to the action requirements of your business and forwards the message as it is if it is empty."
    },
    "method": {
      "label": "Method"
    },
    "path": {
      "label": "URL Path",
      "desc": "The URL path for this Action.<br/>This path will be appended to the Connector's <code>url</code> configuration to form the full URL address.Template with variables is allowed in this option. For example, <code>/room/{'{'}$room_no{'}'}</code>"
    },
    "max_retries": {
      "label": "Max Retries"
    }
  },
  "emqx_ee_bridge_cassandra": {
    "keyspace": {
      "desc": "Keyspace name to connect to.",
      "label": "Keyspace"
    },
    "servers": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port][,Host2:Port]`.<br/>The Cassandra default port 9042 is used if `[:Port]` is not specified.",
      "label": "Servers"
    },
    "cql": {
      "desc": "CQL Template",
      "label": "CQL Template"
    }
  },
  "emqx_ee_bridge_clickhouse": {
    "batch_value_separator": {
      "desc": "The default value ',' works for the VALUES format. You can also use other separator if other format is specified. See [INSERT INTO Statement](https://clickhouse.com/docs/en/sql-reference/statements/insert-into).",
      "label": "Batch Value Separator"
    },
    "connect_timeout": {
      "desc": "The timeout when connecting to the Clickhouse server.",
      "label": "Clickhouse Timeout"
    },
    "url": {
      "desc": "The HTTP URL to the Clickhouse server that you want to connect to (for example http://myhostname:8123)",
      "label": "Server URL"
    },
    "sql": {
      "desc": "The template string can contain ${'{'}field{'}'} placeholders for message metadata and payload field. Make sure that the inserted values are formatted and escaped correctly. [Prepared Statement](https://docs.emqx.com/en/enterprise/v5.0/data-integration/data-bridges.html#Prepared-Statement) is not supported.",
      "label": "SQL Template"
    }
  },
  "emqx_ee_bridge_dynamo": {
    "template": {
      "desc": "Template, the default value is empty. When this value is empty the whole message will be stored in the database.<br>The template can be any valid json with placeholders and make sure all keys for table are here, example:<br>  `{'{'}\"id\" : \"${'{'}id{'}'}\", \"clientid\" : \"${'{'}clientid{'}'}\", \"data\" : \"${'{'}payload.data{'}'}\"{'}'}`",
      "label": "Template"
    },
    "url": {
      "desc": "The url of DynamoDB endpoint.",
      "label": "DynamoDB Endpoint"
    },
    "table": {
      "desc": "DynamoDB Table.",
      "label": "Table "
    },
    "aws_access_key_id": {
      "desc": "Access Key ID for connecting to DynamoDB.",
      "label": "AWS Access Key ID"
    },
    "aws_secret_access_key": {
      "desc": "AWS Secret Access Key for connecting to DynamoDB.",
      "label": "AWS Secret Access Key"
    }
  },
  "emqx_ee_bridge_gcp_pubsub": {
    "connect_timeout": {
      "desc": "The timeout when connecting to the HTTP server.",
      "label": "Connect Timeout"
    },
    "max_retries": {
      "desc": "Max retry times if an error occurs when sending a request.",
      "label": "Max Retries"
    },
    "payload_template": {
      "desc": "The template for formatting the outgoing messages.  If undefined, will send all the available context in JSON format.",
      "label": "Payload Template"
    },
    "pipelining": {
      "desc": "A positive integer. Whether to send HTTP requests continuously, when set to 1, it means that after each HTTP request is sent, you need to wait for the server to return and then continue to send the next request.",
      "label": "HTTP Pipelining"
    },
    "pool_size": {
      "desc": "The pool size.",
      "label": "Pool Size"
    },
    "pubsub_topic": {
      "desc": "The GCP PubSub topic to publish messages to.",
      "label": "GCP PubSub Topic"
    },
    "request_timeout": {
      "desc": "Deprecated: Configure the request timeout in the buffer settings.",
      "label": "Request Timeout"
    },
    "service_account_json": {
      "desc": "JSON containing the GCP Service Account credentials to be used with PubSub.<br/>When a GCP Service Account is created (as described in https://developers.google.com/identity/protocols/oauth2/service-account#creatinganaccount), you have the option of downloading the credentials in JSON form.  That's the file needed.",
      "label": "GCP Service Account Credentials"
    },
    "pull_max_messages": {
      "desc": "The maximum number of messages to retrieve from GCP PubSub in a single pull request.\n The actual number may be less than the specified value.",
      "label": "Maximum Messages to Pull"
    },
    "topic_mapping": {
      "desc": "Defines the mapping between GCP PubSub topics and MQTT topics. Must contain at least one item.",
      "label": "Topic Mapping"
    },
    "consumer_pubsub_topic": {
      "desc": "GCP PubSub topic to consume from.",
      "label": "GCP PubSub Topic"
    },
    "consumer_mqtt_topic": {
      "desc": "Local topic to which consumed GCP PubSub messages should be published to.",
      "label": "MQTT Topic"
    },
    "consumer_qos": {
      "desc": "MQTT QoS used to publish messages consumed from GCP PubSub.",
      "label": "QoS"
    },
    "attributes_template": {
      "desc": "The template for formatting the outgoing message attributes.  Undefined values will be rendered as empty string values.  Empty keys are removed from the attribute map.",
      "label": "Attributes Template"
    },
    "ordering_key_template": {
      "desc": "The template for formatting the outgoing message ordering key.  Undefined values will be rendered as empty string values.  This value will not be added to the message if it's empty.",
      "label": "Ordering Key template"
    }
  },
  "emqx_ee_bridge_hstreamdb": {
    "payload": {
      "desc": "The payload to be forwarded to the HStreamDB. Placeholders supported.",
      "label": "Payload"
    },
    "pool_size": {
      "desc": "HStreamDB Pool Size.",
      "label": "HStreamDB Pool Size"
    },
    "stream": {
      "label": "HStreamDB Stream Name"
    },
    "url": {
      "label": "HStreamDB Server URL",
      "desc": "HStreamDB Server URL. Using gRPC http server address."
    },
    "record_template": {
      "desc": "The HStream record template to be forwarded to the HStreamDB. Placeholders supported.<br />NOTE: When you use `raw record` template (which means the data is not a valid JSON), you should use `read` or `subscription` in HStream to get the data.",
      "label": "HStream Record Template"
    },
    "stream_name": {
      "label": "HStreamDB Stream Name"
    },
    "partition_key": {
      "label": "HStreamDB Partition Key",
      "desc": "HStreamDB Partition Key. Placeholders supported."
    },
    "grpc_timeout": {
      "label": "HStreamDB gRPC Timeout"
    }
  },
  "emqx_ee_bridge_influxdb": {
    "write_syntax": {
      "desc": "Conf of InfluxDB line protocol to write data points. It is a text-based format that provides the measurement, tag set, field set, and timestamp of a data point, and placeholder supported. See also [InfluxDB 2.3 Line Protocol](https://docs.influxdata.com/influxdb/v2.3/reference/syntax/line-protocol/) and [InfluxDB 1.8 Line Protocol](https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_tutorial/) <br/>TLDR:<br/>```<measurement>[,<tag_key>=<tag_value>[,<tag_key>=<tag_value>]] <field_key>=<field_value>[,<field_key>=<field_value>] [<timestamp>]```<br/>Please note that a placeholder for an integer value must be annotated with a suffix `i`. For example `${'{'}payload.int_value{'}'}i`.",
      "label": "Write Syntax"
    },
    "bucket": {
      "desc": "InfluxDB bucket name.",
      "label": "Bucket"
    },
    "database": {
      "desc": "InfluxDB database.",
      "label": "Database"
    },
    "influxdb_api_v1": {
      "desc": "InfluxDB's protocol. Support InfluxDB v1.8 and before.",
      "label": "HTTP API Protocol"
    },
    "influxdb_api_v2": {
      "desc": "InfluxDB's protocol. Support InfluxDB v2.0 and after.",
      "label": "HTTP API V2 Protocol"
    },
    "org": {
      "desc": "Organization name of InfluxDB.",
      "label": "Organization"
    },
    "password": {
      "desc": "InfluxDB password.",
      "label": "Password"
    },
    "precision": {
      "desc": "InfluxDB time precision.",
      "label": "Time Precision"
    },
    "protocol": {
      "desc": "InfluxDB's protocol. HTTP API or HTTP API V2.",
      "label": "Protocol"
    },
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.</br><br/>A host entry has the following form: `Host[:Port]`.</br><br/>The InfluxDB default port 8086 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "token": {
      "desc": "InfluxDB token.",
      "label": "Token"
    },
    "username": {
      "desc": "InfluxDB username.",
      "label": "Username"
    },
    "parameters": {
      "label": "Version of InfluxDB"
    },
    "influxdb_type": {
      "label": "Version of InfluxDB"
    }
  },
  "emqx_ee_bridge_kafka": {
    "connect_timeout": {
      "desc": "Maximum wait time for TCP connection establishment (including authentication time if enabled).",
      "label": "Connect Timeout"
    },
    "min_metadata_refresh_interval": {
      "desc": "Minimum time interval the client has to wait before refreshing Kafka broker and topic metadata. Setting too small value may add extra load on Kafka.",
      "label": "Min Metadata Refresh Interval"
    },
    "kafka_producer": {
      "desc": "Kafka Producer configuration.",
      "label": "Kafka Producer"
    },
    "producer_buffer": {
      "desc": "Configure producer message buffer.<br/>Tell Kafka producer how to buffer messages when EMQX has more messages to send than Kafka can keep up, or when Kafka is down.",
      "label": "Message Buffer"
    },
    "sndbuf": {
      "desc": "Fine tune the socket send buffer. The default value is tuned for high throughput.",
      "label": "Socket Send Buffer Size"
    },
    "consumer_offset_commit_interval_seconds": {
      "desc": "Defines the time interval between two offset commit requests sent for each consumer group.",
      "label": "Offset Commit Interval"
    },
    "consumer_max_batch_bytes": {
      "desc": "Set how many bytes to pull from Kafka in each fetch request. Please note that if the configured value is smaller than the message size in Kafka, it may negatively impact the fetch performance.",
      "label": "Fetch Bytes"
    },
    "recbuf": {
      "desc": "Fine tune the socket receive buffer. The default value is tuned for high throughput.",
      "label": "Socket Receive Buffer Size"
    },
    "consumer_topic_mapping": {
      "desc": "Defines the mapping between Kafka topics and MQTT topics. Must contain at least one item.",
      "label": "Topic Mapping"
    },
    "producer_kafka_opts": {
      "desc": "Kafka producer configs.",
      "label": "Kafka Producer"
    },
    "topic": {
      "desc": "Kafka topic",
      "label": "Kafka Topic"
    },
    "consumer_kafka_topic": {
      "desc": "Kafka topic to consume from.",
      "label": "Kafka Topic"
    },
    "auth_username_password": {
      "desc": "Username/password based authentication.",
      "label": "Username/password Auth"
    },
    "auth_sasl_password": {
      "desc": "SASL authentication password.",
      "label": "Password"
    },
    "kafka_message_timestamp": {
      "desc": "Which timestamp to use. The timestamp is expected to be a millisecond precision Unix epoch which can be in string format, e.g. <code>1661326462115</code> or <code>'1661326462115'</code>. When the desired data field for this template is not found, or if the found data is not a valid integer, the current system timestamp will be used.",
      "label": "Message Timestamp"
    },
    "mode": {
      "desc": "Message buffer mode.<br/><code>memory</code>: Buffer all messages in memory. The messages will be lost in case of EMQX node restart<br/><code>disk</code>: Buffer all messages on disk. The messages on disk are able to survive EMQX node restart.<br/><code>hybrid</code>: Buffer message in memory first, when up to certain limit (see <code>segment_bytes</code> config for more information), then start offloading messages to disk, Like <code>memory</code> mode, the messages will be lost in case of EMQX node restart.",
      "label": "Buffer Mode"
    },
    "qos": {
      "desc": "MQTT QoS used to publish messages consumed from Kafka.",
      "label": "QoS"
    },
    "consumer_key_encoding_mode": {
      "desc": "Defines how the key from the Kafka message is encoded before being forwarded via MQTT.<br/><code>none</code> Uses the key from the Kafka message unchanged.  Note: in this case, the key must be a valid UTF-8 string.<br/><code>base64</code> Uses base-64 encoding on the received key.",
      "label": "Key Encoding Mode"
    },
    "auth_gssapi_kerberos": {
      "desc": "Use GSSAPI/Kerberos authentication.",
      "label": "GSSAPI/Kerberos"
    },
    "consumer_mqtt_opts": {
      "desc": "Local MQTT message publish.",
      "label": "MQTT publish"
    },
    "kerberos_principal": {
      "desc": "SASL GSSAPI authentication Kerberos principal. For example <code>client_name{'@'}MY.KERBEROS.REALM.MYDOMAIN.COM</code>, NOTE: The realm in use has to be configured in /etc/krb5.conf in EMQX nodes.",
      "label": "Kerberos Principal"
    },
    "socket_opts": {
      "desc": "Extra socket options.",
      "label": "Socket Options"
    },
    "consumer_mqtt_topic": {
      "desc": "Local topic to which consumed Kafka messages should be published to.",
      "label": "MQTT Topic"
    },
    "consumer_offset_reset_policy": {
      "desc": "Defines from which offset a consumer should start fetching when there is no commit history or when the commit history becomes invalid.",
      "label": "Offset Reset Policy"
    },
    "partition_count_refresh_interval": {
      "desc": "The time interval for Kafka producer to discover increased number of partitions.<br/>After the number of partitions is increased in Kafka, EMQX will start taking the <br/>discovered partitions into account when dispatching messages per <code>partition_strategy</code>.",
      "label": "Partition Count Refresh Interval"
    },
    "max_batch_bytes": {
      "desc": "Maximum bytes to collect in a Kafka message batch. Most of the Kafka brokers default to a limit of 1 MB batch size. EMQX's default value is less than 1 MB in order to compensate Kafka message encoding overheads (especially when each individual message is very small). When a single message is over the limit, it is still sent (as a single element batch).",
      "label": "Max Batch Bytes"
    },
    "required_acks": {
      "desc": "Required acknowledgements for Kafka partition leader to wait for its followers before it sends back the acknowledgement to EMQX Kafka producer<br/><code>all_isr</code>: Require all in-sync replicas to acknowledge.<br/><code>leader_only</code>: Require only the partition-leader's acknowledgement.<br/><code>none</code>: No need for Kafka to acknowledge at all.",
      "label": "Required Acks"
    },
    "metadata_request_timeout": {
      "desc": "Maximum wait time when fetching metadata from Kafka.",
      "label": "Metadata Request Timeout"
    },
    "nodelay": {
      "desc": "When set to `true`, TCP buffer is sent as soon as possible. Otherwise, the OS kernel may buffer small TCP packets for a while (40 ms by default).",
      "label": "No Delay"
    },
    "authentication": {
      "desc": "Authentication configs.",
      "label": "Authentication"
    },
    "memory_overload_protection": {
      "desc": "Applicable when buffer mode is set to <code>memory</code><br/>EMQX will drop old buffered messages under high memory pressure. The high memory threshold is defined in config <code>sysmon.os.sysmem_high_watermark</code>. NOTE: This config only works on Linux.",
      "label": "Memory Overload Protection"
    },
    "mechanism": {
      "desc": "SASL authentication mechanism.",
      "label": "Mechanism"
    },
    "payload_template": {
      "desc": "The template for transforming the incoming Kafka message.  By default, it will use JSON format to serialize inputs from the Kafka message.  Such fields are:<br/><code>headers</code>: an object containing string key-value pairs.<br/><code>key</code>: Kafka message key (uses the chosen key encoding).<br/><code>offset</code>: offset for the message.<br/><code>topic</code>: Kafka topic.<br/><code>ts</code>: message timestamp.<br/><code>ts_type</code>: message timestamp type, which is one of <code>create</code>, <code>append</code> or <code>undefined</code>.<br/><code>value</code>: Kafka message value (uses the chosen value encoding).",
      "label": "MQTT Payload Template"
    },
    "consumer_opts": {
      "desc": "Local MQTT publish and Kafka consumer configs.",
      "label": "MQTT to Kafka"
    },
    "kafka_consumer": {
      "desc": "Kafka Consumer configuration.",
      "label": "Kafka Consumer"
    },
    "consumer_value_encoding_mode": {
      "desc": "Defines how the value from the Kafka message is encoded before being forwarded via MQTT.<br/><code>none</code> Uses the value from the Kafka message unchanged.  Note: in this case, the value must be a valid UTF-8 string.<br/><code>base64</code> Uses base-64 encoding on the received value.",
      "label": "Value Encoding Mode"
    },
    "per_partition_limit": {
      "desc": "Number of bytes allowed to buffer for each Kafka partition. When this limit is exceeded, old messages will be dropped in a trade for credits for new messages to be buffered.",
      "label": "Per-partition Buffer Limit"
    },
    "bootstrap_hosts": {
      "desc": "A comma separated list of Kafka <code>host[:port]</code> endpoints to bootstrap the client. Default port number is 9092.",
      "label": "Bootstrap Hosts"
    },
    "consumer_max_rejoin_attempts": {
      "desc": "Maximum number of times allowed for a member to re-join the group. If the consumer group can not reach balance after this configured number of attempts, the consumer group member will restart after a delay.",
      "label": "Max Rejoin Attempts"
    },
    "key": {
      "desc": "Template to render Kafka message key. If the template is rendered into a NULL value (i.e. there is no such data field in Rule Engine context) then Kafka's <code>NULL</code> (but not empty string) is used.",
      "label": "Message Key"
    },
    "kafka_message": {
      "desc": "Template to render a Kafka message.",
      "label": "Kafka Message Template"
    },
    "mqtt_topic": {
      "desc": "MQTT topic or topic as data source (bridge input).  Should not configure this if the bridge is used as a rule action.",
      "label": "MQTT Topic"
    },
    "value": {
      "desc": "Template to render Kafka message value. If the template is rendered into a NULL value (i.e. there is no such data field in Rule Engine context) then Kafka's <code>NULL</code> (but not empty string) is used.",
      "label": "Message Value"
    },
    "partition_strategy": {
      "desc": "Partition strategy is to tell the producer how to dispatch messages to Kafka partitions.<br/><code>random</code>: Randomly pick a partition for each message<br/><code>key_dispatch</code>: Hash Kafka message key to a partition number",
      "label": "Partition Strategy"
    },
    "segment_bytes": {
      "desc": "Applicable when buffer mode is set to <code>disk</code> or <code>hybrid</code>.<br/>This value is to specify the size of each on-disk buffer file.",
      "label": "Segment File Bytes"
    },
    "consumer_kafka_opts": {
      "desc": "Kafka consumer configs.",
      "label": "Kafka Consumer"
    },
    "max_inflight": {
      "desc": "Maximum number of batches allowed for Kafka producer (per-partition) to send before receiving acknowledgement from Kafka. Greater value typically means better throughput. However, there can be a risk of message reordering when this value is greater than 1.",
      "label": "Max Inflight"
    },
    "auth_sasl_username": {
      "desc": "SASL authentication username.",
      "label": "Username"
    },
    "kerberos_keytab_file": {
      "desc": "SASL GSSAPI authentication Kerberos keytab file path. NOTE: This file has to be placed in EMQX nodes, and the EMQX service runner user requires read permission.",
      "label": "Kerberos Keytab File"
    },
    "compression": {
      "desc": "Compression method.",
      "label": "Compression"
    },
    "query_mode": {
      "desc": "Query mode. Optional 'sync/async', default 'async'.",
      "label": "Query Mode"
    },
    "sync_query_timeout": {
      "desc": "This parameter defines the timeout limit for synchronous queries. It applies only when the bridge query mode is configured to 'sync'.",
      "label": "Synchronous Query Timeout"
    },
    "tcp_keepalive": {
      "desc": "Enable TCP keepalive for connector.<br/>The value is a comma-separated list of three numbers in the format of `Idle,Interval,Probes`<br/>- Idle: The number of seconds of idle time needed by a connection before the server begins to send out keep-alive probes (default on Linux is 7200).<br/>- Interval: The number of seconds between TCP keep-alive probes (default on Linux is 75).<br/>- Probes: The maximum number of TCP keep-alive probes to send before considering the connection as closed if no response is received from the other end (default on Linux is 9).<br/>For example \"240,30,5\" means: send TCP keepalive probes after 240 seconds of idle time, and send probes every 30 second. If there are no responses for 5 consecutive attempts, the connection should be closed.<br/>Default: 'none'",
      "label": "TCP Keepalive"
    },
    "kafka_headers": {
      "desc": "Please provide a placeholder to be used as Kafka Headers<br/>e.g. <code>${'{'}pub_props{'}'}</code><br/>Notice that the value of the placeholder must either be an object:<br/>`{'{'}\"foo\": \"bar\"{'}'}`<br/>",
      "label": "Kafka Headers"
    },
    "kafka_ext_headers": {
      "desc": "Please provide more key-value pairs for Kafka headers.<br/>The key-value pairs here will be combined with the value of `kafka_headers` field before sending to Kafka.",
      "label": "More Kafka Headers"
    },
    "kafka_header_value_encode_mode": {
      "desc": "Kafka headers value encode mode:<br/>- `NONE`: only add binary values to Kafka headers;<br/>- `JSON`: only add JSON values to Kafka headers, and encode it to JSON strings before sending.",
      "label": "Kafka Header Value Encode Mode"
    },
    "kafka_ext_header_key": {
      "desc": "Key of the Kafka header. Placeholders in the format of <code>${'{'}var{'}'}</code> are supported.",
      "label": "Kafka Header Key"
    },
    "kafka_ext_header_value": {
      "desc": "Value of the Kafka header. Placeholders in the format of <code>${'{'}var{'}'}</code> are supported.",
      "label": "Kafka Header Value"
    }
  },
  "emqx_ee_bridge_mongodb": {
    "collection": {
      "desc": "The collection where data will be stored into",
      "label": "Collection"
    },
    "payload_template": {
      "desc": "The template for formatting the outgoing messages.  If undefined, rule engine will use JSON format to serialize all visible inputs, such as clientid, topic, payload etc.",
      "label": "Payload template"
    },
    "auth_source": {
      "desc": "Database name associated with the user's credentials.",
      "label": "Auth Source"
    },
    "connect_timeout": {
      "desc": "The duration to attempt a connection before timing out.",
      "label": "Connect Timeout"
    },
    "heartbeat_period": {
      "desc": "Controls when the driver checks the state of the MongoDB deployment. Specify the interval between checks, counted from the end of the previous check until the beginning of the next one. If the number of connections is increased (which will happen, for example, if you increase the pool size), you may need to increase this period as well to avoid creating too many log entries in the MongoDB log file.",
      "label": "Heartbeat period"
    },
    "local_threshold": {
      "desc": "The size of the latency window for selecting among multiple suitable MongoDB instances.",
      "label": "Local Threshold"
    },
    "max_overflow": {
      "desc": "The maximum number of additional workers that can be created when all workers in the pool are busy. This helps to manage temporary spikes in workload by allowing more concurrent connections to the MongoDB server.",
      "label": "Max Overflow Workers"
    },
    "min_heartbeat_period": {
      "desc": "Controls the minimum amount of time to wait between heartbeats.",
      "label": "Minimum Heartbeat Period"
    },
    "overflow_check_period": {
      "desc": "Period for checking if there are more workers than configured (\"overflow\").",
      "label": "Overflow Check Period"
    },
    "overflow_ttl": {
      "desc": "Period of time before workers that exceed the configured pool size (\"overflow\") to be terminated.",
      "label": "Overflow TTL"
    },
    "r_mode": {
      "desc": "Read mode.",
      "label": "Read Mode"
    },
    "replica_set_name": {
      "desc": "Name of the replica set.",
      "label": "Replica Set Name"
    },
    "rs_mongo_type": {
      "desc": "Replica set. Must be set to 'rs' when MongoDB server is running in 'replica set' mode.",
      "label": "Replica set"
    },
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The MongoDB default port 27017 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "server_selection_timeout": {
      "desc": "Specifies how long to block for server selection before throwing an exception.",
      "label": "Server Selection Timeout"
    },
    "servers": {
      "desc": "A Node list for Cluster to connect to. The nodes should be separated with commas, such as: `Node[,Node].`<br/>For each Node should be: The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The MongoDB default port 27017 is used if `[:Port]` is not specified.",
      "label": "Servers"
    },
    "sharded_mongo_type": {
      "desc": "Sharded cluster. Must be set to 'sharded' when MongoDB server is running in 'sharded' mode.",
      "label": "Sharded cluster"
    },
    "single_mongo_type": {
      "desc": "Standalone instance. Must be set to 'single' when MongoDB server is running in standalone mode.",
      "label": "Standalone instance"
    },
    "socket_timeout": {
      "desc": "The duration to attempt to send or to receive on a socket before the attempt times out.",
      "label": "Socket Timeout"
    },
    "srv_record": {
      "desc": "Use DNS SRV record.",
      "label": "Srv Record"
    },
    "w_mode": {
      "desc": "Write mode.",
      "label": "Write Mode"
    },
    "wait_queue_timeout": {
      "desc": "The maximum duration that a worker can wait for a connection to become available.",
      "label": "Wait Queue Timeout"
    },
    "use_legacy_protocol": {
      "desc": "Whether to use MongoDB's legacy protocol for communicating with the database. The default is to attempt to automatically determine if the newer protocol is supported.",
      "label": "Use Legacy Protocol"
    },
    "mongo_type": {
      "label": "MongoDB Mode"
    },
    "parameters": {
      "label": "MongoDB Mode"
    }
  },
  "emqx_ee_bridge_mysql": {
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The MySQL default port 3306 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "sql": {
      "desc": "SQL Template",
      "label": "SQL Template"
    }
  },
  "emqx_ee_bridge_pgsql": {
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The default port 5432 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "sql": {
      "desc": "SQL Template",
      "label": "SQL Template"
    }
  },
  "emqx_ee_bridge_redis": {
    "command_template": {
      "desc": "Redis command template used to export messages. Each list element stands for a command name or its argument.<br/>For example, to push payloads in a Redis list by key `msgs`, the elements should be the following:<br/>`rpush`, `msgs`, `${'{'}payload{'}'}`.",
      "label": "Redis Command Template"
    },
    "cluster": {
      "desc": "Cluster mode. Must be set to 'cluster' when Redis server is running in clustered mode.",
      "label": "Cluster Mode"
    },
    "database": {
      "desc": "Redis database ID.",
      "label": "Database ID"
    },
    "sentinel": {
      "desc": "Sentinel mode. Must be set to 'sentinel' when Redis server is running in sentinel mode.",
      "label": "Sentinel Mode"
    },
    "sentinel_desc": {
      "desc": "The cluster name in Redis sentinel mode.",
      "label": "Cluster Name"
    },
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The Redis default port 6379 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "servers": {
      "desc": "A Node list for Cluster to connect to. The nodes should be separated with commas, such as: `Node[,Node].`<br/>For each Node should be: The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The Redis default port 6379 is used if `[:Port]` is not specified.",
      "label": "Servers"
    },
    "single": {
      "desc": "Single mode. Must be set to 'single' when Redis server is running in single mode.",
      "label": "Single Mode"
    },
    "parameters": {
      "label": "Redis Mode"
    },
    "redis_type": {
      "label": "Redis Mode"
    }
  },
  "emqx_ee_bridge_rocketmq": {
    "template": {
      "desc": "Template, the default value is empty. When this value is empty the whole message will be stored in the RocketMQ.<br>The template can be any valid string with placeholders, example:<br>- ${'{'}id{'}'}, ${'{'}username{'}'}, ${'{'}clientid{'}'}, ${'{'}timestamp{'}'}<br>- {'{'}\"id\" : ${'{'}id{'}'}, \"username\" : ${'{'}username{'}'}{'}'}",
      "label": "Template"
    },
    "refresh_interval": {
      "desc": "RocketMQ Topic Route Refresh Interval.",
      "label": "Topic Route Refresh Interval"
    },
    "security_token": {
      "desc": "RocketMQ Server Security Token",
      "label": "Security Token"
    },
    "send_buffer": {
      "desc": "The socket send buffer size of the RocketMQ driver client.",
      "label": "Send Buffer Size"
    },
    "servers": {
      "label": "Server Host",
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The RocketMQ default port 9876 is used if `[:Port]` is not specified."
    },
    "topic": {
      "desc": "RocketMQ Topic",
      "label": "RocketMQ Topic"
    },
    "access_key": {
      "label": "AccessKey",
      "desc": "RocketMQ server `accessKey`."
    },
    "secret_key": {
      "label": "SecretKey",
      "desc": "RocketMQ server `secretKey`."
    },
    "sync_timeout": {
      "label": "Sync Timeout",
      "desc": "Timeout of RocketMQ driver synchronous call."
    }
  },
  "emqx_ee_bridge_tdengine": {
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The TDengine default port 6041 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "sql": {
      "desc": "SQL Template",
      "label": "SQL Template"
    }
  },
  "emqx_ee_bridge_sqlserver": {
    "server": {
      "label": "Server Host",
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The SQL Server default port 1433 is used if `[:Port]` is not specified."
    },
    "driver": {
      "label": "SQL Server Driver Name",
      "desc": "SQL Server Driver Name"
    },
    "sql": {
      "label": "SQL Template",
      "desc": "SQL Template"
    }
  },
  "emqx_ee_bridge_iotdb": {
    "authentication": {
      "desc": "Authentication configuration",
      "label": "Authentication"
    },
    "auth_basic": {
      "desc": "Parameters for basic authentication.",
      "label": "Basic auth params"
    },
    "auth_basic_username": {
      "desc": "The username as configured at the IoTDB REST interface",
      "label": "HTTP Basic Auth Username"
    },
    "auth_basic_password": {
      "desc": "The password as configured at the IoTDB REST interface",
      "label": "HTTP Basic Auth Password"
    },
    "base_url": {
      "desc": "The base URL of the external IoTDB service's REST interface.<br/> The URL has the following form `http://Host:Port`.",
      "label": "IoTDB REST Service Base URL"
    },
    "is_aligned": {
      "desc": "Whether to align the timeseries",
      "label": "Align Timeseries"
    },
    "device_id": {
      "desc": "A fixed device name this data should be inserted for. If empty it must either be set in the rule action, the message itself, or it will be extracted from the topic.",
      "label": "Device ID"
    },
    "iotdb_version": {
      "desc": "The version of the IoTDB system to connect to.",
      "label": "IoTDB Version"
    },
    "max_retries": {
      "desc": "HTTP request max retry times if failed.",
      "label": "HTTP Request Max Retries"
    },
    "request_timeout": {
      "desc": "HTTP request timeout.",
      "label": "HTTP Request Timeout"
    },
    "enable_pipelining": {
      "desc": "A positive integer. Whether to send HTTP requests continuously, when set to 1, it means that after each HTTP request is sent, you need to wait for the server to return and then continue to send the next request.",
      "label": "HTTP Pipelining"
    },
    "pool_type": {
      "desc": "The type of the pool. Can be one of `random`, `hash`.",
      "label": "Pool Type"
    },
    "connect_timeout": {
      "desc": "The timeout when connecting to the HTTP server.",
      "label": "Connect Timeout"
    },
    "data": {
      "label": "Write Data"
    },
    "timestamp": {
      "label": "Timestamp",
      "desc": "Supports the use of ${'{'}var{'}'} format as a placeholder, which should be in timestamp format. You can also use the following special characters to insert system time:</br>- `now`: Current millisecond-level timestamp</br>- `now_ms`: Current millisecond-level timestamp</br>- `now_us`: Current microsecond-level timestamp</br>- `now_ns`: Current nanosecond-level timestamp"
    },
    "measurement": {
      "label": "Measurement",
      "desc": "Supports the use of ${'{'}var{'}'} format as a placeholder."
    },
    "data_type": {
      "label": "Data Type",
      "desc": "Data Type, supports the use of ${'{'}var{'}'} format as a placeholder."
    },
    "value": {
      "label": "Value",
      "desc": "The value to be inserted, supports the use of ${'{'}var{'}'} format as a placeholder."
    }
  },
  "emqx_ee_bridge_opents": {
    "server": {
      "desc": "The URL of OpenTSDB endpoint.",
      "label": "URL"
    },
    "summary": {
      "desc": "Whether to return summary information.",
      "label": "Summary"
    },
    "details": {
      "desc": "Whether to return detailed information.",
      "label": "Details"
    }
  },
  "emqx_ee_bridge_oracle": {
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The Oracle Database default port 1521 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "sid": {
      "desc": "SID for Oracle Database.",
      "label": "Oracle Database SID"
    },
    "sql": {
      "desc": "SQL Template. The template string can contain placeholders for message metadata and payload field. The placeholders are inserted without any checking and special formatting, so it is important to ensure that the inserted values are formatted and escaped correctly.",
      "label": "SQL Template"
    },
    "service_name": {
      "label": "Oracle Database Service Name"
    }
  },
  "emqx_ee_bridge_rabbitmq": {
    "server": {
      "desc": "The RabbitMQ server address that you want to connect to (for example, localhost).",
      "label": "Server"
    },
    "port": {
      "desc": "The port number on which the RabbitMQ server is listening (default is 5672).",
      "label": "Port"
    },
    "username": {
      "desc": "The username used to authenticate with the RabbitMQ server.",
      "label": "Username"
    },
    "pool_size": {
      "desc": "The size of the connection pool.",
      "label": "Pool Size"
    },
    "timeout": {
      "desc": "The timeout for waiting on the connection to be established.",
      "label": "Connection Timeout"
    },
    "virtual_host": {
      "desc": "The virtual host to use when connecting to the RabbitMQ server.",
      "label": "Virtual Host"
    },
    "heartbeat": {
      "desc": "The interval for sending heartbeat messages to the RabbitMQ server.",
      "label": "Heartbeat"
    },
    "auto_reconnect": {
      "desc": "The interval for attempting to reconnect to the RabbitMQ server if the connection is lost.",
      "label": "Auto Reconnect"
    },
    "exchange": {
      "desc": "The name of the RabbitMQ exchange where the messages will be sent.",
      "label": "Exchange"
    },
    "exchange_type": {
      "desc": "The type of the RabbitMQ exchange (direct, fanout, or topic).",
      "label": "Exchange Type"
    },
    "routing_key": {
      "desc": "The routing key used to route messages to the correct queue in the RabbitMQ exchange.",
      "label": "Routing Key"
    },
    "delivery_mode": {
      "desc": "The delivery mode for messages published to RabbitMQ. Delivery mode `non_persistent` is suitable for messages that don't require persistence across RabbitMQ restarts, whereas delivery mode `persistent` is designed for messages that must survive RabbitMQ restarts.",
      "label": "Message Delivery Mode"
    },
    "payload_template": {
      "desc": "The template for formatting the payload of the message before sending it to RabbitMQ. Template placeholders, such as ${'{'}field1.sub_field{'}'}, will be substituted with the respective field's value. When left empty, the entire input message will be used as the payload, formatted as a JSON text. This behavior is equivalent to specifying ${'{'}.{'}'} as the payload template.",
      "label": "Payload Template"
    },
    "publish_confirmation_timeout": {
      "desc": "The timeout for waiting for RabbitMQ to confirm message publication when using publisher confirms.",
      "label": "Publish Confirmation Timeout"
    },
    "wait_for_publish_confirmations": {
      "desc": "A boolean value that indicates whether to wait for RabbitMQ to confirm message publication when using publisher confirms.",
      "label": "Wait for Publish Confirmations"
    }
  },
  "emqx_ee_bridge_pulsar": {
    "auth_basic": {
      "desc": "Parameters for basic authentication.",
      "label": "Auth Parameters (Basic)"
    },
    "auth_basic_password": {
      "desc": "Basic authentication password.",
      "label": "Password"
    },
    "auth_basic_username": {
      "desc": "Basic authentication username.",
      "label": "Username"
    },
    "auth_token": {
      "desc": "Parameters for token authentication.",
      "label": "Auth Parameters (Token)"
    },
    "authentication_jwt": {
      "desc": "JWT authentication token.",
      "label": "JWT"
    },
    "authentication": {
      "desc": "Authentication configurations.",
      "label": "Authentication"
    },
    "buffer_memory_overload_protection": {
      "desc": "Applicable when buffer mode is set to <code>memory</code><br/>EMQX will drop old buffered messages under high memory pressure. The high memory threshold is defined in config <code>sysmon.os.sysmem_high_watermark</code>. NOTE: This config only works on Linux.",
      "label": "Memory Overload Protection"
    },
    "buffer_mode": {
      "desc": "Message buffer mode:<br/><code>memory</code>: Buffer all messages in memory. The messages will be lost if EMQX node restarts.<br/><code>disk</code>: Buffer all messages on disk. The messages are designed to persist even during an EMQX node restart.<br/><code>hybrid</code>: Buffer message in memory first, when up to a certain limit (see <code>segment_bytes</code> config for more information), then start offloading messages to disk, the messages will be lost in if EMQX node restarts.",
      "label": "Buffer Mode"
    },
    "buffer_per_partition_limit": {
      "desc": "Number of bytes allowed to buffer for each Pulsar partition. When the message limit is exceeded, older messages will be selectively dropped to allocate buffer space for new messages.",
      "label": "Per-partition Buffer Limit"
    },
    "buffer_segment_bytes": {
      "desc": "Applicable when buffer mode is set to <code>disk</code> or <code>hybrid</code>.<br/>This value is to specify the size of each on-disk buffer file.",
      "label": "Segment File Bytes"
    },
    "batch_size": {
      "desc": "Specify the maximum number of individual requests to be batched within a Pulsar message.",
      "label": "Batch Size"
    },
    "buffer": {
      "desc": "Configure producer message buffer.<br/>This is to inform Pulsar producer how to proceed when EMQX has more messages to send than Pulsar can handle, or when Pulsar is down",
      "label": "Message Buffer"
    },
    "compression": {
      "desc": "Compression method.",
      "label": "Compression"
    },
    "message_key": {
      "desc": "Template to render Pulsar message key.",
      "label": "Message Key"
    },
    "max_batch_bytes": {
      "desc": "Specify the limit on the number of bytes collected in a batch. EMQX has set the default value to less than 5 MB to account for encoding overheads especially when handling small messages. If a single message exceeds the batch size limit, it will still be sent as a single-element batch, indicating it will be treated as a separate batch containing only that specific message.",
      "label": "Max Batch Bytes"
    },
    "message_opts": {
      "desc": "Template to render a Pulsar message.",
      "label": "Pulsar Message Template"
    },
    "pulsar_message": {
      "desc": "Template to render a Pulsar message.",
      "label": "Pulsar Message Template"
    },
    "pulsar_topic": {
      "desc": "Pulsar topic name",
      "label": "Pulsar Topic Name"
    },
    "retention_period": {
      "desc": "Specify the duration to buffer messages when disconnected from the Pulsar broker. Longer times increase memory/disk usage.",
      "label": "Retention Period"
    },
    "send_buffer": {
      "desc": "Fine tune the socket send buffer. The default value is tuned for high throughput.",
      "label": "Socket Send Buffer Size"
    },
    "strategy": {
      "desc": "Choose how messages are dispatched to Pulsar partitions:<br/><code>random</code>: Messages are randomly assigned to partitions.<br/><code>roundrobin</code>: Messages are evenly distributed across available producers.<br/><code>key_dispatch</code>: Partitions to be selected are hashed and stored in the Pulsar message key of the first message in a batch.",
      "label": "Partition Strategy"
    },
    "sync_timeout": {
      "desc": "Maximum wait time for receipt in synchronously publishing.",
      "label": "Sync Publish Timeout"
    },
    "message_value": {
      "desc": "Template to render Pulsar message value.",
      "label": "Message Value"
    },
    "servers": {
      "desc": "Specify the Pulsar Server URL in the format of <code>scheme://host[:port]</code> for the client to connect to. Multiple servers should be added as a comma-separated list. The supported schemes are <code>pulsar://</code> (default) and <code>pulsar+ssl://</code>. The default port is 6650.",
      "label": "Servers"
    },
    "connect_timeout": {
      "desc": "Maximum wait time for TCP connection establishment (including authentication time if enabled).",
      "label": "Connect Timeout"
    }
  },
  "emqx_ee_bridge_azure_event_hub": {
    "bootstrap_hosts": {
      "desc": "A comma separated list of Azure Event Hubs Kafka <code>host[:port]</code> namespace endpoints to bootstrap the client.  Default port number is 9093.",
      "label": "Bootstrap Hosts"
    },
    "connect_timeout": {
      "desc": "Maximum wait time for TCP connection establishment (including authentication time if enabled).",
      "label": "Connect Timeout"
    },
    "min_metadata_refresh_interval": {
      "desc": "Minimum time interval the client has to wait before refreshing Azure Event Hubs Kafka broker and topic metadata. Setting too small value may add extra load on Azure Event Hubs.",
      "label": "Min Metadata Refresh Interval"
    },
    "metadata_request_timeout": {
      "desc": "Maximum wait time when fetching metadata from Azure Event Hubs.",
      "label": "Metadata Request Timeout"
    },
    "connection_string": {
      "desc": "The Connection String for connecting to Azure Event Hubs. Should be the \"connection string-primary key\" of a Namespace shared access policy.",
      "label": "Connection String"
    },
    "sndbuf": {
      "desc": "Fine tune the socket send buffer. The default value is tuned for high throughput.",
      "label": "Socket Send Buffer Size"
    },
    "recbuf": {
      "desc": "Fine tune the socket receive buffer. The default value is tuned for high throughput.",
      "label": "Socket Receive Buffer Size"
    },
    "tcp_keepalive": {
      "desc": "Enable TCP keepalive for Azure Event Hubs bridge connections.\nThe value is three comma separated numbers in the format of 'Idle,Interval,Probes'\n - Idle: The number of seconds a connection needs to be idle before the server begins to send out keep-alive probes (Linux default 7200).\n - Interval: The number of seconds between TCP keep-alive probes (Linux default 75).\n - Probes: The maximum number of TCP keep-alive probes to send before giving up and killing the connection if no response is obtained from the other end (Linux default 9).\nFor example \"240,30,5\" means: TCP keepalive probes are sent after the connection is idle for 240 seconds, and the probes are sent every 30 seconds until a response is received, if it misses 5 consecutive responses, the connection should be closed.\nDefault: 'none'",
      "label": "TCP keepalive options"
    },
    "topic": {
      "desc": "Event Hub name",
      "label": "Event Hub Name"
    },
    "max_batch_bytes": {
      "desc": "Maximum bytes to collect in an Azure Event Hubs message batch. Most of the Kafka brokers default to a limit of 1 MB batch size. EMQX's default value is less than 1 MB in order to compensate Kafka message encoding overheads (especially when each individual message is very small). When a single message is over the limit, it is still sent (as a single element batch).",
      "label": "Max Batch Bytes"
    },
    "partition_strategy": {
      "desc": "Partition strategy is to tell the producer how to dispatch messages to Azure Event Hubs partitions.\n\n<code>random</code>: Randomly pick a partition for each message\n<code>key_dispatch</code>: Hash Azure Event Hubs message key to a partition number",
      "label": "Partition Strategy"
    },
    "required_acks": {
      "desc": "Required acknowledgements for Azure Event Hubs partition leader to wait for its followers before it sends back the acknowledgement to EMQX Azure Event Hubs producer\n\n<code>all_isr</code>: Require all in-sync replicas to acknowledge.\n<code>leader_only</code>: Require only the partition-leader's acknowledgement.",
      "label": "Required Acks"
    },
    "kafka_headers": {
      "desc": "Please provide a placeholder to be used as Azure Event Hubs Headers<br/>\ne.g. <code>${'{'}pub_props{'}'}</code><br/>\nNotice that the value of the placeholder must either be an object:\n<code>{'{'}\\\"foo\\\": \\\"bar\\\"{'}'}</code>",
      "label": "Azure Event Hubs Headers"
    },
    "kafka_ext_headers": {
      "desc": "Please provide more key-value pairs for Azure Event Hubs headers<br/>The key-value pairs here will be combined with the value of <code>kafka_headers</code> field before sending to Azure Event Hubs.",
      "label": "Extra Azure Event Hubs headers"
    },
    "kafka_header_value_encode_mode": {
      "desc": "Azure Event Hubs headers value encode mode<br/>\n - NONE: only add binary values to Azure Event Hubs headers;<br/>\n - JSON: only add JSON values to Azure Event Hubs headers,\nand encode it to JSON strings before sending.",
      "label": "Azure Event Hubs headers value encode mode"
    },
    "partition_count_refresh_interval": {
      "desc": "The time interval for Azure Event Hubs producer to discover increased number of partitions.\nAfter the number of partitions is increased in Azure Event Hubs, EMQX will start taking the\ndiscovered partitions into account when dispatching messages per <code>partition_strategy</code>.",
      "label": "Partition Count Refresh Interval"
    },
    "max_inflight": {
      "desc": "Maximum number of batches allowed for Azure Event Hubs producer (per-partition) to send before receiving acknowledgement from Azure Event Hubs. Greater value typically means better throughput. However, there can be a risk of message reordering when this value is greater than 1.",
      "label": "Max Inflight"
    },
    "query_mode": {
      "desc": "Query mode. Optional 'sync/async', default 'async'.",
      "label": "Query mode"
    },
    "sync_query_timeout": {
      "desc": "This parameter defines the timeout limit for synchronous queries. It applies only when the bridge query mode is configured to 'sync'.",
      "label": "Synchronous Query Timeout"
    },
    "key": {
      "desc": "Template to render Azure Event Hubs message key. If the template is rendered into a NULL value (i.e. there is no such data field in Rule Engine context) then Azure Event Hubs's <code>NULL</code> (but not empty string) is used.",
      "label": "Message Key"
    },
    "value": {
      "desc": "Template to render Azure Event Hubs message value. If the template is rendered into a NULL value (i.e. there is no such data field in Rule Engine context) then Azure Event Hubs's <code>NULL</code> (but not empty string) is used.",
      "label": "Message Value"
    },
    "timestamp": {
      "desc": "Which timestamp to use. The timestamp is expected to be a millisecond precision Unix epoch which can be in string format, e.g. <code>1661326462115</code> or <code>'1661326462115'</code>. When the desired data field for this template is not found, or if the found data is not a valid integer, the current system timestamp will be used.",
      "label": "Message Timestamp"
    },
    "kafka_ext_header_key": {
      "desc": "Key of the Azure Event Hubs header. Placeholders in format of ${'{'}var{'}'} are supported.",
      "label": "Key"
    },
    "kafka_ext_header_value": {
      "desc": "Value of the Azure Event Hubs header. Placeholders in format of ${'{'}var{'}'} are supported.",
      "label": "Value"
    },
    "mode": {
      "desc": "Message buffer mode.\n\n<code>memory</code>: Buffer all messages in memory. The messages will be lost in case of EMQX node restart\n<code>disk</code>: Buffer all messages on disk. The messages on disk are able to survive EMQX node restart.\n<code>hybrid</code>: Buffer message in memory first, when up to certain limit (see <code>segment_bytes</code> config for more information), then start offloading messages to disk, Like <code>memory</code> mode, the messages will be lost in case of EMQX node restart.",
      "label": "Buffer Mode"
    },
    "per_partition_limit": {
      "desc": "Number of bytes allowed to buffer for each Azure Event Hubs partition. When this limit is exceeded, old messages will be dropped in a trade for credits for new messages to be buffered.",
      "label": "Per-partition Buffer Limit"
    },
    "segment_bytes": {
      "desc": "Applicable when buffer mode is set to <code>disk</code> or <code>hybrid</code>.\nThis value is to specify the size of each on-disk buffer file.",
      "label": "Segment File Bytes"
    },
    "memory_overload_protection": {
      "desc": "Applicable when buffer mode is set to <code>memory</code>\nEMQX will drop old buffered messages under high memory pressure. The high memory threshold is defined in config <code>sysmon.os.sysmem_high_watermark</code>. NOTE: This config only works on Linux.",
      "label": "Memory Overload Protection"
    },
    "nodelay": {
      "desc": "When set to `true`, TCP buffer is sent as soon as possible. Otherwise, the OS kernel may buffer small TCP packets for a while (40 ms by default).",
      "label": "No Delay"
    }
  },
  "emqx_ee_bridge_kinesis": {
    "pool_size": {
      "desc": "The pool size.",
      "label": "Pool Size"
    },
    "payload_template": {
      "desc": "The template for formatting the outgoing messages.  If undefined, will send all the available context in JSON format.",
      "label": "Payload template"
    },
    "aws_access_key_id": {
      "desc": "Access Key ID for connecting to Amazon Kinesis.",
      "label": "AWS Access Key ID"
    },
    "aws_secret_access_key": {
      "desc": "AWS Secret Access Key for connecting to Amazon Kinesis.",
      "label": "AWS Secret Access Key"
    },
    "endpoint": {
      "desc": "The url of Amazon Kinesis endpoint.",
      "label": "Amazon Kinesis Endpoint"
    },
    "stream_name": {
      "desc": "The Amazon Kinesis Stream to publish messages to.",
      "label": "Amazon Kinesis Stream"
    },
    "partition_key": {
      "desc": "The Amazon Kinesis Partition Key associated to published message. Placeholders in format of ${'{'}var{'}'} are supported.",
      "label": "Partition key"
    },
    "max_retries": {
      "desc": "Max retry times if an error occurs when sending a request.",
      "label": "Max Retries"
    }
  },
  "emqx_ee_bridge_greptimedb": {
    "write_syntax": {
      "desc": "Conf of GreptimeDB gRPC protocol to write data points. Write syntax is a text-based format that provides the measurement, tag set, field set, and timestamp of a data point, and placeholder supported, which is the same as InfluxDB line protocol.\nSee also [InfluxDB 2.3 Line Protocol](https://docs.influxdata.com/influxdb/v2.3/reference/syntax/line-protocol/) and\n[InfluxDB 1.8 Line Protocol](https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_tutorial/) </br>\nTLDR:</br>\n```\n<measurement>[,<tag_key>=<tag_value>[,<tag_key>=<tag_value>]] <field_key>=<field_value>[,<field_key>=<field_value>] [<timestamp>]\n```\nPlease note that a placeholder for an integer value must be annotated with a suffix `i`. For example `${'{'}payload.int_value{'}'}i`.",
      "label": "Write Syntax"
    },
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to. A host entry has the following form: `Host[:Port]`.",
      "label": "Server Host"
    },
    "dbname": {
      "desc": "GreptimeDB database.",
      "label": "Database"
    },
    "greptimedb": {
      "desc": "GreptimeDB's protocol. Support GreptimeDB v1.8 and before.",
      "label": "HTTP API Protocol"
    },
    "username": {
      "desc": "GreptimeDB username.",
      "label": "Username"
    },
    "precision": {
      "desc": "GreptimeDB time precision.",
      "label": "Time Precision"
    },
    "protocol": {
      "desc": "GreptimeDB's protocol. gRPC API.",
      "label": "Protocol"
    }
  },
  "emqx_ee_bridge_syskeeper_proxy": {
    "listen": {
      "label": "Listen Address",
      "desc": "The listening address for this Syskeeper proxy server"
    },
    "acceptors": {
      "label": "Acceptors",
      "desc": "The number of the acceptors"
    },
    "handshake_timeout": {
      "label": "Handshake Timeout",
      "desc": "The maximum to wait for the handshake when a connection is created"
    }
  },
  "emqx_ee_bridge_syskeeper_forwarder": {
    "server": {
      "label": "Server",
      "desc": "The address of the Syskeeper proxy server"
    },
    "ack_mode": {
      "label": "ACK Mode",
      "desc": "Specify whether the proxy server should reply with an acknowledgement for the message forwarding, can be:<br>- need_ack <br>- no_ack <br>"
    },
    "ack_timeout": {
      "label": "ACK Timeout",
      "desc": "The maximum time to wait for an acknowledgement from the proxy server"
    },
    "target_topic": {
      "desc": "The topic for the forwarded message",
      "label": "Topic"
    },
    "target_qos": {
      "desc": "The QoS for the forwarded message, left blank for the original topic",
      "label": "QoS"
    },
    "template": {
      "desc": "Message template. Placeholders supported.",
      "label": "Message Template"
    }
  },
  "emqx_ee_bridge_elasticsearch": {
    "base_url": {
      "desc": "The REST URL of the ElasticSearch service.",
      "label": "URL"
    },
    "routing": {
      "desc": "Specifies the shard in the index where the document should be stored. If left blank, Elasticsearch decides.",
      "label": "Routing"
    },
    "parameters": {
      "label": "Action"
    },
    "action": {
      "label": "Action"
    },
    "wait_for_active_shards": {
      "desc": "Number of shard replicas that must be active before proceeding.\nSet to \"all\" or any positive integer, up to the total number of shards in the index (number_of_replicas+1).\nDefault: 1, i.e., the primary shard.",
      "label": "Wait For Active Shards"
    },
    "index": {
      "desc": "Name of the index or index alias to perform the operation on, supports ${'{'}var{'}'} format placeholders.",
      "label": "Index Name"
    },
    "id": {
      "desc": "Unique identifier for the document within the index, supports ${'{'}var{'}'} format placeholders. If ID is not specified, it is autogenerated by Elasticsearch.",
      "label": "Document ID"
    },
    "doc": {
      "desc": "Custom document template, supports ${'{'}var{'}'} format placeholders, must be convertible to a JSON object.\nFor example, `{'{'} \"field\": \"${'{'}payload.field{'}'}\" {'}'}`, or `${'{'}payload{'}'}`.",
      "label": "Document Template"
    },
    "overwrite": {
      "desc": "Overwrites the document if it already exists, otherwise the write will fail.",
      "label": "Overwrite Document"
    },
    "max_retries": {
      "desc": "Maximum number of retries when a request fails.",
      "label": "Max Retries"
    }
  }
}