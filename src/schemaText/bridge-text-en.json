{
  "emqx_connector_schema_lib": {
    "auto_reconnect": {
      "desc": "Deprecated. Enable automatic reconnect to the database.",
      "label": "Deprecated. Auto Reconnect Database"
    },
    "password": {
      "desc": "The password associated with the bridge, used for authentication with the external database.",
      "label": "Password"
    },
    "pool_size": {
      "desc": "Size of the connection pool towards the bridge target service.",
      "label": "Connection Pool Size"
    },
    "prepare_statement": {
      "desc": "Key-value list of SQL prepared statements.",
      "label": "SQL Prepared Statements List"
    },
    "ssl": {
      "desc": "SSL connection settings.",
      "label": "Enable SSL"
    },
    "username": {
      "desc": "The username associated with the bridge in the external database used for authentication or identification purposes.",
      "label": "Username"
    },
    "database": {
      "desc": "Database name.",
      "label": "Database Name"
    }
  },
  "emqx_ee_bridge_cassa": {
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to Cassandra. All MQTT 'PUBLISH' messages with the topic<br/>matching the local_topic will be forwarded.</br><br/>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also local_topic is<br/>configured, then both the data got from the rule and the MQTT messages that match local_topic<br/>will be forwarded.",
      "label": "Local Topic"
    },
    "keyspace": {
      "desc": "Keyspace name to connect to.",
      "label": "Keyspace"
    },
    "servers": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port][,Host2:Port]`.<br/>The Cassandra default port 9042 is used if `[:Port]` is not specified.",
      "label": "Servers"
    },
    "cql": {
      "desc": "CQL Template",
      "label": "CQL Template"
    }
  },
  "emqx_ee_bridge_clickhouse": {
    "batch_value_separator": {
      "desc": "The default value ',' works for the VALUES format. You can also use other separator if other format is specified. See [INSERT INTO Statement](https://clickhouse.com/docs/en/sql-reference/statements/insert-into).",
      "label": "Batch Value Separator"
    },
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to Clickhouse. All MQTT 'PUBLISH' messages with the topic<br/>matching the local_topic will be forwarded.</br><br/>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also local_topic is<br/>configured, then both the data got from the rule and the MQTT messages that match local_topic<br/>will be forwarded.",
      "label": "Local Topic"
    },
    "connect_timeout": {
      "desc": "The timeout when connecting to the Clickhouse server.",
      "label": "Clickhouse Timeout"
    },
    "url": {
      "desc": "The HTTP URL to the Clickhouse server that you want to connect to (for example http://myhostname:8123)",
      "label": "Server URL"
    },
    "sql": {
      "desc": "The template string can contain ${'{'}field{'}'} placeholders for message metadata and payload field. Make sure that the inserted values are formatted and escaped correctly. [Prepared Statement](https://docs.emqx.com/en/enterprise/v5.0/data-integration/data-bridges.html#Prepared-Statement) is not supported.",
      "label": "SQL Template"
    }
  },
  "emqx_ee_bridge_dynamo": {
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to DynamoDB. All MQTT `PUBLISH` messages with the topic<br/>matching the `local_topic` will be forwarded.</br><br/>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also `local_topic` is<br/>configured, then both the data got from the rule and the MQTT messages that match `local_topic`<br/>will be forwarded.",
      "label": "Local Topic"
    },
    "template": {
      "desc": "Template, the default value is empty. When this value is empty the whole message will be stored in the database.<br>The template can be any valid json with placeholders and make sure all keys for table are here, example:<br>  `{'{'}\"id\" : \"${'{'}id{'}'}\", \"clientid\" : \"${'{'}clientid{'}'}\", \"data\" : \"${'{'}payload.data{'}'}\"{'}'}`",
      "label": "Template"
    },
    "url": {
      "desc": "The url of DynamoDB endpoint.",
      "label": "DynamoDB Endpoint"
    },
    "table": {
      "desc": "DynamoDB Table.",
      "label": "Table "
    },
    "aws_access_key_id": {
      "desc": "Access Key ID for connecting to DynamoDB.",
      "label": "AWS Access Key ID"
    },
    "aws_secret_access_key": {
      "desc": "AWS Secret Access Key for connecting to DynamoDB.",
      "label": "AWS Secret Access Key"
    }
  },
  "emqx_ee_bridge_gcp_pubsub": {
    "connect_timeout": {
      "desc": "The timeout when connecting to the HTTP server.",
      "label": "Connect Timeout"
    },
    "desc_config": {
      "desc": "Configuration for a GCP PubSub bridge.",
      "label": "GCP PubSub Bridge Configuration"
    },
    "desc_name": {
      "desc": "Bridge name, used as a human-readable description of the bridge.",
      "label": "Bridge Name"
    },
    "desc_type": {
      "desc": "The Bridge Type",
      "label": "Bridge Type"
    },
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to GCP PubSub. All MQTT 'PUBLISH' messages with the topic<br/>matching `local_topic` will be forwarded.</br><br/>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also local_topic is<br/>configured, then both the data got from the rule and the MQTT messages that match local_topic<br/>will be forwarded.",
      "label": "Local Topic"
    },
    "max_retries": {
      "desc": "Max retry times if an error occurs when sending a request.",
      "label": "Max Retries"
    },
    "payload_template": {
      "desc": "The template for formatting the outgoing messages.  If undefined, will send all the available context in JSON format.",
      "label": "Payload Template"
    },
    "pipelining": {
      "desc": "A positive integer. Whether to send HTTP requests continuously, when set to 1, it means that after each HTTP request is sent, you need to wait for the server to return and then continue to send the next request.",
      "label": "HTTP Pipelining"
    },
    "pool_size": {
      "desc": "The pool size.",
      "label": "Pool Size"
    },
    "pubsub_topic": {
      "desc": "The GCP PubSub topic to publish messages to.",
      "label": "GCP PubSub Topic"
    },
    "request_timeout": {
      "desc": "Deprecated: Configure the request timeout in the buffer settings.",
      "label": "Request Timeout"
    },
    "service_account_json": {
      "desc": "JSON containing the GCP Service Account credentials to be used with PubSub.<br/>When a GCP Service Account is created (as described in https://developers.google.com/identity/protocols/oauth2/service-account#creatinganaccount), you have the option of downloading the credentials in JSON form.  That's the file needed.",
      "label": "GCP Service Account Credentials"
    }
  },
  "emqx_ee_bridge_hstreamdb": {
    "direction": {
      "desc": "The direction of this bridge, MUST be 'egress'",
      "label": "Bridge Direction"
    },
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to the HStreamDB. All MQTT 'PUBLISH' messages with the topic<br/>matching the local_topic will be forwarded.</br><br/>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also local_topic is<br/>configured, then both the data got from the rule and the MQTT messages that match local_topic<br/>will be forwarded.",
      "label": "Local Topic"
    },
    "payload": {
      "desc": "The payload to be forwarded to the HStreamDB. Placeholders supported.",
      "label": "Payload"
    },
    "pool_size": {
      "desc": "HStreamDB Pool Size.",
      "label": "HStreamDB Pool Size"
    },
    "stream": {
      "label": "HStreamDB Stream Name"
    },
    "url": {
      "label": "HStreamDB Server URL",
      "desc": "HStreamDB Server URL. Using gRPC http server address."
    },
    "record_template": {
      "desc": "The HStream record template to be forwarded to the HStreamDB. Placeholders supported.<br />NOTE: When you use `raw record` template (which means the data is not a valid JSON), you should use `read` or `subscription` in HStream to get the data.",
      "label": "HStream Record Template"
    },
    "stream_name": {
      "label": "HStreamDB Stream Name"
    },
    "partition_key": {
      "label": "HStreamDB Partition Key",
      "desc": "HStreamDB Partition Key. Placeholders supported."
    },
    "grpc_timeout": {
      "label": "HStreamDB gRPC Timeout"
    }
  },
  "emqx_ee_bridge_influxdb": {
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to the InfluxDB. All MQTT 'PUBLISH' messages with the topic<br/>matching the local_topic will be forwarded.</br><br/>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also local_topic is<br/>configured, then both the data got from the rule and the MQTT messages that match local_topic<br/>will be forwarded.",
      "label": "Local Topic"
    },
    "write_syntax": {
      "desc": "Conf of InfluxDB line protocol to write data points. It is a text-based format that provides the measurement, tag set, field set, and timestamp of a data point, and placeholder supported.<br/>See also [InfluxDB 2.3 Line Protocol](https://docs.influxdata.com/influxdb/v2.3/reference/syntax/line-protocol/) and<br/>[InfluxDB 1.8 Line Protocol](https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_tutorial/) </br><br/>TLDR:</br><br/>```<br/><measurement>[,<tag_key>=<tag_value>[,<tag_key>=<tag_value>]] <field_key>=<field_value>[,<field_key>=<field_value>] [<timestamp>]<br/>```<br/>Please note that a placeholder for an integer value must be annotated with a suffix `i`. For example `${'{'}payload.int_value{'}'}i`.",
      "label": "Write Syntax"
    },
    "bucket": {
      "desc": "InfluxDB bucket name.",
      "label": "Bucket"
    },
    "database": {
      "desc": "InfluxDB database.",
      "label": "Database"
    },
    "influxdb_api_v1": {
      "desc": "InfluxDB's protocol. Support InfluxDB v1.8 and before.",
      "label": "HTTP API Protocol"
    },
    "influxdb_api_v2": {
      "desc": "InfluxDB's protocol. Support InfluxDB v2.0 and after.",
      "label": "HTTP API V2 Protocol"
    },
    "org": {
      "desc": "Organization name of InfluxDB.",
      "label": "Organization"
    },
    "password": {
      "desc": "InfluxDB password.",
      "label": "Password"
    },
    "precision": {
      "desc": "InfluxDB time precision.",
      "label": "Time Precision"
    },
    "protocol": {
      "desc": "InfluxDB's protocol. HTTP API or HTTP API V2.",
      "label": "Protocol"
    },
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.</br><br/>A host entry has the following form: `Host[:Port]`.</br><br/>The InfluxDB default port 8086 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "token": {
      "desc": "InfluxDB token.",
      "label": "Token"
    },
    "username": {
      "desc": "InfluxDB username.",
      "label": "Username"
    }
  },
  "emqx_ee_bridge_kafka": {
    "connect_timeout": {
      "desc": "Maximum wait time for TCP connection establishment (including authentication time if enabled).",
      "label": "Connect Timeout"
    },
    "producer_opts": {
      "desc": "Local MQTT data source and Kafka bridge configs.",
      "label": "MQTT to Kafka"
    },
    "min_metadata_refresh_interval": {
      "desc": "Minimum time interval the client has to wait before refreshing Kafka broker and topic metadata. Setting too small value may add extra load on Kafka.",
      "label": "Min Metadata Refresh Interval"
    },
    "kafka_producer": {
      "desc": "Kafka Producer configuration.",
      "label": "Kafka Producer"
    },
    "producer_buffer": {
      "desc": "Configure producer message buffer.<br/>Tell Kafka producer how to buffer messages when EMQX has more messages to send than Kafka can keep up, or when Kafka is down.",
      "label": "Message Buffer"
    },
    "socket_send_buffer": {
      "desc": "Fine tune the socket send buffer. The default value is tuned for high throughput.",
      "label": "Socket Send Buffer Size"
    },
    "consumer_offset_commit_interval_seconds": {
      "desc": "Defines the time interval between two offset commit requests sent for each consumer group.",
      "label": "Offset Commit Interval"
    },
    "consumer_max_batch_bytes": {
      "desc": "Set how many bytes to pull from Kafka in each fetch request. Please note that if the configured value is smaller than the message size in Kafka, it may negatively impact the fetch performance.",
      "label": "Fetch Bytes"
    },
    "socket_receive_buffer": {
      "desc": "Fine tune the socket receive buffer. The default value is tuned for high throughput.",
      "label": "Socket Receive Buffer Size"
    },
    "consumer_topic_mapping": {
      "desc": "Defines the mapping between Kafka topics and MQTT topics. Must contain at least one item.",
      "label": "Topic Mapping"
    },
    "producer_kafka_opts": {
      "desc": "Kafka producer configs.",
      "label": "Kafka Producer"
    },
    "kafka_topic": {
      "desc": "Kafka topic",
      "label": "Kafka Topic"
    },
    "consumer_kafka_topic": {
      "desc": "Kafka topic to consume from.",
      "label": "Kafka Topic"
    },
    "auth_username_password": {
      "desc": "Username/password based authentication.",
      "label": "Username/password Auth"
    },
    "auth_sasl_password": {
      "desc": "SASL authentication password.",
      "label": "Password"
    },
    "kafka_message_timestamp": {
      "desc": "Which timestamp to use. The timestamp is expected to be a millisecond precision Unix epoch which can be in string format, e.g. <code>1661326462115</code> or <code>'1661326462115'</code>. When the desired data field for this template is not found, or if the found data is not a valid integer, the current system timestamp will be used.",
      "label": "Message Timestamp"
    },
    "buffer_mode": {
      "desc": "Message buffer mode.<br/><code>memory</code>: Buffer all messages in memory. The messages will be lost in case of EMQX node restart<br/><code>disk</code>: Buffer all messages on disk. The messages on disk are able to survive EMQX node restart.<br/><code>hybrid</code>: Buffer message in memory first, when up to certain limit (see <code>segment_bytes</code> config for more information), then start offloading messages to disk, Like <code>memory</code> mode, the messages will be lost in case of EMQX node restart.",
      "label": "Buffer Mode"
    },
    "qos": {
      "desc": "MQTT QoS used to publish messages consumed from Kafka.",
      "label": "QoS"
    },
    "consumer_key_encoding_mode": {
      "desc": "Defines how the key from the Kafka message is encoded before being forwarded via MQTT.<br/><code>none</code> Uses the key from the Kafka message unchanged.  Note: in this case, the key must be a valid UTF-8 string.<br/><code>base64</code> Uses base-64 encoding on the received key.",
      "label": "Key Encoding Mode"
    },
    "auth_gssapi_kerberos": {
      "desc": "Use GSSAPI/Kerberos authentication.",
      "label": "GSSAPI/Kerberos"
    },
    "consumer_mqtt_opts": {
      "desc": "Local MQTT message publish.",
      "label": "MQTT publish"
    },
    "auth_kerberos_principal": {
      "desc": "SASL GSSAPI authentication Kerberos principal. For example <code>client_name{'@'}MY.KERBEROS.REALM.MYDOMAIN.COM</code>, NOTE: The realm in use has to be configured in /etc/krb5.conf in EMQX nodes.",
      "label": "Kerberos Principal"
    },
    "socket_opts": {
      "desc": "Extra socket options.",
      "label": "Socket Options"
    },
    "consumer_mqtt_topic": {
      "desc": "Local topic to which consumed Kafka messages should be published to.",
      "label": "MQTT Topic"
    },
    "consumer_offset_reset_policy": {
      "desc": "Defines from which offset a consumer should start fetching when there is no commit history or when the commit history becomes invalid.",
      "label": "Offset Reset Policy"
    },
    "partition_count_refresh_interval": {
      "desc": "The time interval for Kafka producer to discover increased number of partitions.<br/>After the number of partitions is increased in Kafka, EMQX will start taking the <br/>discovered partitions into account when dispatching messages per <code>partition_strategy</code>.",
      "label": "Partition Count Refresh Interval"
    },
    "max_batch_bytes": {
      "desc": "Maximum bytes to collect in a Kafka message batch. Most of the Kafka brokers default to a limit of 1 MB batch size. EMQX's default value is less than 1 MB in order to compensate Kafka message encoding overheads (especially when each individual message is very small). When a single message is over the limit, it is still sent (as a single element batch).",
      "label": "Max Batch Bytes"
    },
    "required_acks": {
      "desc": "Required acknowledgements for Kafka partition leader to wait for its followers before it sends back the acknowledgement to EMQX Kafka producer<br/><code>all_isr</code>: Require all in-sync replicas to acknowledge.<br/><code>leader_only</code>: Require only the partition-leader's acknowledgement.<br/><code>none</code>: No need for Kafka to acknowledge at all.",
      "label": "Required Acks"
    },
    "metadata_request_timeout": {
      "desc": "Maximum wait time when fetching metadata from Kafka.",
      "label": "Metadata Request Timeout"
    },
    "socket_nodelay": {
      "desc": "When set to 'true', TCP buffer is sent as soon as possible. Otherwise, the OS kernel may buffer small TCP packets for a while (40 ms by default).",
      "label": "No Delay"
    },
    "authentication": {
      "desc": "Authentication configs.",
      "label": "Authentication"
    },
    "buffer_memory_overload_protection": {
      "desc": "Applicable when buffer mode is set to <code>memory</code><br/>EMQX will drop old buffered messages under high memory pressure. The high memory threshold is defined in config <code>sysmon.os.sysmem_high_watermark</code>. NOTE: This config only works on Linux.",
      "label": "Memory Overload Protection"
    },
    "auth_sasl_mechanism": {
      "desc": "SASL authentication mechanism.",
      "label": "Mechanism"
    },
    "payload_template": {
      "desc": "The template for transforming the incoming Kafka message.  By default, it will use JSON format to serialize inputs from the Kafka message.  Such fields are:<br/><code>headers</code>: an object containing string key-value pairs.<br/><code>key</code>: Kafka message key (uses the chosen key encoding).<br/><code>offset</code>: offset for the message.<br/><code>topic</code>: Kafka topic.<br/><code>ts</code>: message timestamp.<br/><code>ts_type</code>: message timestamp type, which is one of <code>create</code>, <code>append</code> or <code>undefined</code>.<br/><code>value</code>: Kafka message value (uses the chosen value encoding).",
      "label": "MQTT Payload Template"
    },
    "consumer_opts": {
      "desc": "Local MQTT publish and Kafka consumer configs.",
      "label": "MQTT to Kafka"
    },
    "kafka_consumer": {
      "desc": "Kafka Consumer configuration.",
      "label": "Kafka Consumer"
    },
    "consumer_value_encoding_mode": {
      "desc": "Defines how the value from the Kafka message is encoded before being forwarded via MQTT.<br/><code>none</code> Uses the value from the Kafka message unchanged.  Note: in this case, the value must be a valid UTF-8 string.<br/><code>base64</code> Uses base-64 encoding on the received value.",
      "label": "Value Encoding Mode"
    },
    "buffer_per_partition_limit": {
      "desc": "Number of bytes allowed to buffer for each Kafka partition. When this limit is exceeded, old messages will be dropped in a trade for credits for new messages to be buffered.",
      "label": "Per-partition Buffer Limit"
    },
    "bootstrap_hosts": {
      "desc": "A comma separated list of Kafka <code>host[:port]</code> endpoints to bootstrap the client. Default port number is 9092.",
      "label": "Bootstrap Hosts"
    },
    "consumer_max_rejoin_attempts": {
      "desc": "Maximum number of times allowed for a member to re-join the group. If the consumer group can not reach balance after this configured number of attempts, the consumer group member will restart after a delay.",
      "label": "Max Rejoin Attempts"
    },
    "kafka_message_key": {
      "desc": "Template to render Kafka message key. If the template is rendered into a NULL value (i.e. there is no such data field in Rule Engine context) then Kafka's <code>NULL</code> (but not empty string) is used.",
      "label": "Message Key"
    },
    "kafka_message": {
      "desc": "Template to render a Kafka message.",
      "label": "Kafka Message Template"
    },
    "mqtt_topic": {
      "desc": "MQTT topic or topic as data source (bridge input).  Should not configure this if the bridge is used as a rule action.",
      "label": "MQTT Topic"
    },
    "kafka_message_value": {
      "desc": "Template to render Kafka message value. If the template is rendered into a NULL value (i.e. there is no such data field in Rule Engine context) then Kafka's <code>NULL</code> (but not empty string) is used.",
      "label": "Message Value"
    },
    "partition_strategy": {
      "desc": "Partition strategy is to tell the producer how to dispatch messages to Kafka partitions.<br/><code>random</code>: Randomly pick a partition for each message<br/><code>key_dispatch</code>: Hash Kafka message key to a partition number",
      "label": "Partition Strategy"
    },
    "buffer_segment_bytes": {
      "desc": "Applicable when buffer mode is set to <code>disk</code> or <code>hybrid</code>.<br/>This value is to specify the size of each on-disk buffer file.",
      "label": "Segment File Bytes"
    },
    "consumer_kafka_opts": {
      "desc": "Kafka consumer configs.",
      "label": "Kafka Consumer"
    },
    "max_inflight": {
      "desc": "Maximum number of batches allowed for Kafka producer (per-partition) to send before receiving acknowledgement from Kafka. Greater value typically means better throughput. However, there can be a risk of message reordering when this value is greater than 1.",
      "label": "Max Inflight"
    },
    "auth_sasl_username": {
      "desc": "SASL authentication username.",
      "label": "Username"
    },
    "auth_kerberos_keytab_file": {
      "desc": "SASL GSSAPI authentication Kerberos keytab file path. NOTE: This file has to be placed in EMQX nodes, and the EMQX service runner user requires read permission.",
      "label": "Kerberos keytab file"
    },
    "compression": {
      "desc": "Compression method.",
      "label": "Compression"
    },
    "query_mode": {
      "desc": "Query mode. Optional 'sync/async', default 'async'.",
      "label": "Query Mode"
    },
    "sync_query_timeout": {
      "desc": "This parameter defines the timeout limit for synchronous queries. It applies only when the bridge query mode is configured to 'sync'.",
      "label": "Synchronous Query Timeout"
    },
    "tcp_keepalive": {
      "desc": "Enable TCP keepalive for Kafka bridge connections.<br/>The value is a comma-separated list of three numbers in the format of `Idle,Interval,Probes`<br/>- Idle: The number of seconds of idle time needed by a connection before the server begins to send out keep-alive probes (default on Linux is 7200).<br/>- Interval: The number of seconds between TCP keep-alive probes (default on Linux is 75).<br/>- Probes: The maximum number of TCP keep-alive probes to send before considering the connection as closed if no response is received from the other end (default on Linux is 9).<br/>For example \"240,30,5\" means: send TCP keepalive probes after 240 seconds of idle time, and send probes every 30 second. If there are no responses for 5 consecutive attempts, the connection should be closed.<br/>Default: 'none'",
      "label": "TCP Keepalive"
    },
    "kafka_headers": {
      "desc": "Please provide a placeholder to be used as Kafka Headers<br/>e.g. <code>${'{'}pub_props{'}'}</code><br/>Notice that the value of the placeholder must either be an object:<br/>`{'{'}\"foo\": \"bar\"{'}'}`<br/>or an array of key-value pairs:<br/>`[{'{'}\"key\": \"foo\", \"value\": \"bar\"{'}'}]`",
      "label": "Kafka Headers"
    },
    "kafka_ext_headers": {
      "desc": "Please provide more key-value pairs for Kafka headers.<br/>The key-value pairs here will be combined with the value of `kafka_headers` field before sending to Kafka.",
      "label": "More Kafka Headers"
    },
    "kafka_header_value_encode_mode": {
      "desc": "Kafka headers value encode mode:<br/>- `NONE`: only add binary values to Kafka headers;<br/>- `JSON`: only add JSON values to Kafka headers, and encode it to JSON strings before sending.",
      "label": "Kafka Header Value Encode Mode"
    },
    "kafka_ext_header_key": {
      "desc": "Key of the Kafka header. Placeholders in the format of <code>${'{'}var{'}'}</code> are supported.",
      "label": "Kafka Header Key"
    },
    "kafka_ext_header_value": {
      "desc": "Value of the Kafka header. Placeholders in the format of <code>${'{'}var{'}'}</code> are supported.",
      "label": "Kafka Header Value"
    }
  },
  "emqx_ee_bridge_mongodb": {
    "collection": {
      "desc": "The collection where data will be stored into",
      "label": "Collection"
    },
    "mongodb_rs_conf": {
      "desc": "MongoDB (Replica Set) configuration",
      "label": "MongoDB (Replica Set) Configuration"
    },
    "mongodb_sharded_conf": {
      "desc": "MongoDB (Sharded) configuration",
      "label": "MongoDB (Sharded) Configuration"
    },
    "mongodb_single_conf": {
      "desc": "MongoDB (Standalone) configuration",
      "label": "MongoDB (Standalone) Configuration"
    },
    "payload_template": {
      "desc": "The template for formatting the outgoing messages.  If undefined, rule engine will use JSON format to serialize all visible inputs, such as clientid, topic, payload etc.",
      "label": "Payload template"
    },
    "auth_source": {
      "desc": "Database name associated with the user's credentials.",
      "label": "Auth Source"
    },
    "connect_timeout": {
      "desc": "The duration to attempt a connection before timing out.",
      "label": "Connect Timeout"
    },
    "desc_rs": {
      "desc": "Settings for replica set.",
      "label": "Setting Replica Set"
    },
    "desc_sharded": {
      "desc": "Settings for sharded cluster.",
      "label": "Setting Sharded Cluster"
    },
    "desc_single": {
      "desc": "Settings for a single MongoDB instance.",
      "label": "Setting Single MongoDB"
    },
    "desc_topology": {
      "desc": "Topology of MongoDB.",
      "label": "Setting Topology"
    },
    "heartbeat_period": {
      "desc": "Controls when the driver checks the state of the MongoDB deployment. Specify the interval between checks, counted from the end of the previous check until the beginning of the next one. If the number of connections is increased (which will happen, for example, if you increase the pool size), you may need to increase this period as well to avoid creating too many log entries in the MongoDB log file.",
      "label": "Heartbeat period"
    },
    "local_threshold": {
      "desc": "The size of the latency window for selecting among multiple suitable MongoDB instances.",
      "label": "Local Threshold"
    },
    "max_overflow": {
      "desc": "The maximum number of additional workers that can be created when all workers in the pool are busy. This helps to manage temporary spikes in workload by allowing more concurrent connections to the MongoDB server.",
      "label": "Max Overflow Workers"
    },
    "min_heartbeat_period": {
      "desc": "Controls the minimum amount of time to wait between heartbeats.",
      "label": "Minimum Heartbeat Period"
    },
    "overflow_check_period": {
      "desc": "Period for checking if there are more workers than configured (\"overflow\").",
      "label": "Overflow Check Period"
    },
    "overflow_ttl": {
      "desc": "Period of time before workers that exceed the configured pool size (\"overflow\") to be terminated.",
      "label": "Overflow TTL"
    },
    "r_mode": {
      "desc": "Read mode.",
      "label": "Read Mode"
    },
    "replica_set_name": {
      "desc": "Name of the replica set.",
      "label": "Replica Set Name"
    },
    "rs_mongo_type": {
      "desc": "Replica set. Must be set to 'rs' when MongoDB server is running in 'replica set' mode.",
      "label": "Replica set"
    },
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The MongoDB default port 27017 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "server_selection_timeout": {
      "desc": "Specifies how long to block for server selection before throwing an exception.",
      "label": "Server Selection Timeout"
    },
    "servers": {
      "desc": "A Node list for Cluster to connect to. The nodes should be separated with commas, such as: `Node[,Node].`<br/>For each Node should be: The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The MongoDB default port 27017 is used if `[:Port]` is not specified.",
      "label": "Servers"
    },
    "sharded_mongo_type": {
      "desc": "Sharded cluster. Must be set to 'sharded' when MongoDB server is running in 'sharded' mode.",
      "label": "Sharded cluster"
    },
    "single_mongo_type": {
      "desc": "Standalone instance. Must be set to 'single' when MongoDB server is running in standalone mode.",
      "label": "Standalone instance"
    },
    "socket_timeout": {
      "desc": "The duration to attempt to send or to receive on a socket before the attempt times out.",
      "label": "Socket Timeout"
    },
    "srv_record": {
      "desc": "Use DNS SRV record.",
      "label": "Srv Record"
    },
    "w_mode": {
      "desc": "Write mode.",
      "label": "Write Mode"
    },
    "wait_queue_timeout": {
      "desc": "The maximum duration that a worker can wait for a connection to become available.",
      "label": "Wait Queue Timeout"
    }
  },
  "emqx_ee_bridge_mysql": {
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to MySQL. All MQTT 'PUBLISH' messages with the topic<br/>matching the local_topic will be forwarded.</br><br/>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also local_topic is<br/>configured, then both the data got from the rule and the MQTT messages that match local_topic<br/>will be forwarded.",
      "label": "Local Topic"
    },
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The MySQL default port 3306 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "sql": {
      "desc": "SQL Template",
      "label": "SQL Template"
    }
  },
  "emqx_ee_bridge_pgsql": {
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to this data bridge. All MQTT 'PUBLISH' messages with the topic<br/>matching the local_topic will be forwarded.</br><br/>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also local_topic is<br/>configured, then both the data got from the rule and the MQTT messages that match local_topic<br/>will be forwarded.",
      "label": "Local Topic"
    },
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The default port 5432 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "sql": {
      "desc": "SQL Template",
      "label": "SQL Template"
    }
  },
  "emqx_ee_bridge_redis": {
    "command_template": {
      "desc": "Redis command template used to export messages. Each list element stands for a command name or its argument.<br/>For example, to push payloads in a Redis list by key `msgs`, the elements should be the following:<br/>`rpush`, `msgs`, `${'{'}payload{'}'}`.",
      "label": "Redis Command Template"
    },
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to Redis. All MQTT 'PUBLISH' messages with the topic<br/>matching the local_topic will be forwarded.</br><br/>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also local_topic is<br/>configured, then both the data got from the rule and the MQTT messages that match local_topic<br/>will be forwarded.",
      "label": "Local Topic"
    },
    "cluster": {
      "desc": "Cluster mode. Must be set to 'cluster' when Redis server is running in clustered mode.",
      "label": "Cluster Mode"
    },
    "database": {
      "desc": "Redis database ID.",
      "label": "Database ID"
    },
    "sentinel": {
      "desc": "Sentinel mode. Must be set to 'sentinel' when Redis server is running in sentinel mode.",
      "label": "Sentinel Mode"
    },
    "sentinel_desc": {
      "desc": "The cluster name in Redis sentinel mode.",
      "label": "Cluster Name"
    },
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The Redis default port 6379 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "servers": {
      "desc": "A Node list for Cluster to connect to. The nodes should be separated with commas, such as: `Node[,Node].`<br/>For each Node should be: The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The Redis default port 6379 is used if `[:Port]` is not specified.",
      "label": "Servers"
    },
    "single": {
      "desc": "Single mode. Must be set to 'single' when Redis server is running in single mode.",
      "label": "Single Mode"
    }
  },
  "emqx_ee_bridge_rocketmq": {
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to RocketMQ. All MQTT `PUBLISH` messages with the topic<br/>matching the `local_topic` will be forwarded.</br><br/>NOTE: if the bridge is used as a rule action, `local_topic` should be left empty otherwise the messages will be duplicated.",
      "label": "Local Topic"
    },
    "template": {
      "desc": "Template, the default value is empty. When this value is empty the whole message will be stored in the RocketMQ.<br>The template can be any valid string with placeholders, example:<br>- ${'{'}id{'}'}, ${'{'}username{'}'}, ${'{'}clientid{'}'}, ${'{'}timestamp{'}'}<br>- {'{'}\"id\" : ${'{'}id{'}'}, \"username\" : ${'{'}username{'}'}{'}'}",
      "label": "Template"
    },
    "refresh_interval": {
      "desc": "RocketMQ Topic Route Refresh Interval.",
      "label": "Topic Route Refresh Interval"
    },
    "security_token": {
      "desc": "RocketMQ Server Security Token",
      "label": "Security Token"
    },
    "send_buffer": {
      "desc": "The socket send buffer size of the RocketMQ driver client.",
      "label": "Send Buffer Size"
    },
    "servers": {
      "label": "Server Host",
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The RocketMQ default port 9876 is used if `[:Port]` is not specified."
    },
    "topic": {
      "desc": "RocketMQ Topic",
      "label": "RocketMQ Topic"
    },
    "access_key": {
      "label": "AccessKey",
      "desc": "RocketMQ server `accessKey`."
    },
    "secret_key": {
      "label": "SecretKey",
      "desc": "RocketMQ server `secretKey`."
    },
    "sync_timeout": {
      "label": "Sync Timeout",
      "desc": "Timeout of RocketMQ driver synchronous call."
    }
  },
  "emqx_ee_bridge_tdengine": {
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to TDengine. All MQTT 'PUBLISH' messages with the topic<br/>matching the local_topic will be forwarded.</br><br/>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also local_topic is<br/>configured, then both the data got from the rule and the MQTT messages that match local_topic<br/>will be forwarded.",
      "label": "Local Topic"
    },
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The TDengine default port 6041 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "sql": {
      "desc": "SQL Template",
      "label": "SQL Template"
    }
  },
  "emqx_ee_bridge_sqlserver": {
    "server": {
      "label": "Server Host",
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The SQL Server default port 1433 is used if `[:Port]` is not specified."
    },
    "driver": {
      "label": "SQL Server Driver Name",
      "desc": "SQL Server Driver Name"
    },
    "sql": {
      "label": "SQL Template",
      "desc": "SQL Template"
    },
    "local_topic": {
      "label": "Local Topic",
      "desc": "The MQTT topic filter to be forwarded to Microsoft SQL Server. All MQTT 'PUBLISH' messages with the topic<br/>matching the local_topic will be forwarded.</br><br/>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also local_topic is<br/>configured, then both the data got from the rule and the MQTT messages that match local_topic<br/>will be forwarded."
    }
  },
  "emqx_ee_bridge_iotdb": {
    "authentication": {
      "desc": "Authentication configuration",
      "label": "Authentication"
    },
    "auth_basic": {
      "desc": "Parameters for basic authentication.",
      "label": "Basic auth params"
    },
    "auth_basic_username": {
      "desc": "The username as configured at the IoTDB REST interface",
      "label": "HTTP Basic Auth Username"
    },
    "auth_basic_password": {
      "desc": "The password as configured at the IoTDB REST interface",
      "label": "HTTP Basic Auth Password"
    },
    "base_url": {
      "desc": "The base URL of the external IoTDB service's REST interface.<br/> The URL has the following form `http://Host:Port`.",
      "label": "IoTDB REST Service Base URL"
    },
    "is_aligned": {
      "desc": "Whether to align the timeseries",
      "label": "Align Timeseries"
    },
    "device_id": {
      "desc": "A fixed device name this data should be inserted for. If empty it must either be set in the rule action, the message itself, or it will be extracted from the topic.",
      "label": "Device ID"
    },
    "iotdb_version": {
      "desc": "The version of the IoTDB system to connect to.",
      "label": "IoTDB Version"
    },
    "max_retries": {
      "desc": "HTTP request max retry times if failed.",
      "label": "HTTP Request Max Retries"
    },
    "request_timeout": {
      "desc": "HTTP request timeout.",
      "label": "HTTP Request Timeout"
    },
    "enable_pipelining": {
      "desc": "A positive integer. Whether to send HTTP requests continuously, when set to 1, it means that after each HTTP request is sent, you need to wait for the server to return and then continue to send the next request.",
      "label": "HTTP Pipelining"
    },
    "pool_type": {
      "desc": "The type of the pool. Can be one of `random`, `hash`.",
      "label": "Pool Type"
    },
    "connect_timeout": {
      "desc": "The timeout when connecting to the HTTP server.",
      "label": "Connect Timeout"
    }
  },
  "emqx_ee_bridge_opents": {
    "server": {
      "desc": "The URL of OpenTSDB endpoint.",
      "label": "URL"
    },
    "summary": {
      "desc": "Whether to return summary information.",
      "label": "Summary"
    },
    "details": {
      "desc": "Whether to return detailed information.",
      "label": "Details"
    }
  },
  "emqx_ee_bridge_oracle": {
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.<br/>A host entry has the following form: `Host[:Port]`.<br/>The Oracle Database default port 1521 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "sid": {
      "desc": "SID for Oracle Database.",
      "label": "Oracle Database SID"
    },
    "local_topic": {
      "desc": "The MQTT topic filter to be forwarded to Oracle Database. All MQTT 'PUBLISH' messages with the topic matching the local_topic will be forwarded.</br>NOTE: if this bridge is used as the action of a rule (EMQX rule engine), and also local_topic is configured, then both the data got from the rule and the MQTT messages that match local_topic will be forwarded.",
      "label": "Server Host"
    },
    "sql": {
      "desc": "SQL Template. The template string can contain placeholders for message metadata and payload field. The placeholders are inserted without any checking and special formatting, so it is important to ensure that the inserted values are formatted and escaped correctly.",
      "label": "SQL Template"
    },
    "service_name": {
      "label": "Oracle Database Service Name"
    }
  },
  "emqx_ee_bridge_rabbitmq": {
    "server": {
      "desc": "The RabbitMQ server address that you want to connect to (for example, localhost).",
      "label": "Server"
    },
    "port": {
      "desc": "The port number on which the RabbitMQ server is listening (default is 5672).",
      "label": "Port"
    },
    "username": {
      "desc": "The username used to authenticate with the RabbitMQ server.",
      "label": "Username"
    },
    "password": {
      "desc": "The password used to authenticate with the RabbitMQ server.",
      "label": "Password"
    },
    "pool_size": {
      "desc": "The size of the connection pool.",
      "label": "Pool Size"
    },
    "timeout": {
      "desc": "The timeout for waiting on the connection to be established.",
      "label": "Connection Timeout"
    },
    "virtual_host": {
      "desc": "The virtual host to use when connecting to the RabbitMQ server.",
      "label": "Virtual Host"
    },
    "heartbeat": {
      "desc": "The interval for sending heartbeat messages to the RabbitMQ server.",
      "label": "Heartbeat"
    },
    "auto_reconnect": {
      "desc": "The interval for attempting to reconnect to the RabbitMQ server if the connection is lost.",
      "label": "Auto Reconnect"
    },
    "exchange": {
      "desc": "The name of the RabbitMQ exchange where the messages will be sent.",
      "label": "Exchange"
    },
    "exchange_type": {
      "desc": "The type of the RabbitMQ exchange (direct, fanout, or topic).",
      "label": "Exchange Type"
    },
    "routing_key": {
      "desc": "The routing key used to route messages to the correct queue in the RabbitMQ exchange.",
      "label": "Routing Key"
    },
    "delivery_mode": {
      "desc": "The delivery mode for messages published to RabbitMQ. Delivery mode `non_persistent` is suitable for messages that don't require persistence across RabbitMQ restarts, whereas delivery mode `persistent` is designed for messages that must survive RabbitMQ restarts.",
      "label": "Message Delivery Mode"
    },
    "payload_template": {
      "desc": "The template for formatting the payload of the message before sending it to RabbitMQ. Template placeholders, such as ${'{'}field1.sub_field{'}'}, will be substituted with the respective field's value. When left empty, the entire input message will be used as the payload, formatted as a JSON text. This behavior is equivalent to specifying ${'{'}.{'}'} as the payload template.",
      "label": "Payload Template"
    },
    "publish_confirmation_timeout": {
      "desc": "The timeout for waiting for RabbitMQ to confirm message publication when using publisher confirms.",
      "label": "Publish Confirmation Timeout"
    },
    "wait_for_publish_confirmations": {
      "desc": "A boolean value that indicates whether to wait for RabbitMQ to confirm message publication when using publisher confirms.",
      "label": "Wait for Publish Confirmations"
    }
  },
  "emqx_ee_bridge_pulsar": {
    "auth_basic": {
      "desc": "Parameters for basic authentication.",
      "label": "Auth Parameters (Basic)"
    },
    "auth_basic_password": {
      "desc": "Basic authentication password.",
      "label": "Password"
    },
    "auth_basic_username": {
      "desc": "Basic authentication username.",
      "label": "Username"
    },
    "auth_token": {
      "desc": "Parameters for token authentication.",
      "label": "Auth Parameters (Token)"
    },
    "authentication_jwt": {
      "desc": "JWT authentication token.",
      "label": "JWT"
    },
    "authentication": {
      "desc": "Authentication configurations.",
      "label": "Authentication"
    },
    "buffer_memory_overload_protection": {
      "desc": "Applicable when buffer mode is set to <code>memory</code><br/>EMQX will drop old buffered messages under high memory pressure. The high memory threshold is defined in config <code>sysmon.os.sysmem_high_watermark</code>. NOTE: This config only works on Linux.",
      "label": "Memory Overload Protection"
    },
    "buffer_mode": {
      "desc": "Message buffer mode:<br/><code>memory</code>: Buffer all messages in memory. The messages will be lost if EMQX node restarts.<br/><code>disk</code>: Buffer all messages on disk. The messages are designed to persist even during an EMQX node restart.<br/><code>hybrid</code>: Buffer message in memory first, when up to a certain limit (see <code>segment_bytes</code> config for more information), then start offloading messages to disk, the messages will be lost in if EMQX node restarts.",
      "label": "Buffer Mode"
    },
    "buffer_per_partition_limit": {
      "desc": "Number of bytes allowed to buffer for each Pulsar partition. When the message limit is exceeded, older messages will be selectively dropped to allocate buffer space for new messages.",
      "label": "Per-partition Buffer Limit"
    },
    "buffer_segment_bytes": {
      "desc": "Applicable when buffer mode is set to <code>disk</code> or <code>hybrid</code>.<br/>This value is to specify the size of each on-disk buffer file.",
      "label": "Segment File Bytes"
    },
    "batch_size": {
      "desc": "Specify the maximum number of individual requests to be batched within a Pulsar message.",
      "label": "Batch Size"
    },
    "buffer": {
      "desc": "Configure producer message buffer.<br/>This is to inform Pulsar producer how to proceed when EMQX has more messages to send than Pulsar can handle, or when Pulsar is down",
      "label": "Message Buffer"
    },
    "compression": {
      "desc": "Compression method.",
      "label": "Compression"
    },
    "message_key": {
      "desc": "Template to render Pulsar message key.",
      "label": "Message Key"
    },
    "local_topic": {
      "desc": "Specify the MQTT topic or topic filter to be used as the data source for the bridge input, or leave blank to use the rule action as data source.",
      "label": "Source MQTT Topic"
    },
    "max_batch_bytes": {
      "desc": "Specify the limit on the number of bytes collected in a batch. EMQX has set the default value to less than 5 MB to account for encoding overheads especially when handling small messages. If a single message exceeds the batch size limit, it will still be sent as a single-element batch, indicating it will be treated as a separate batch containing only that specific message.",
      "label": "Max Batch Bytes"
    },
    "message_opts": {
      "desc": "Template to render a Pulsar message.",
      "label": "Pulsar Message Template"
    },
    "pulsar_message": {
      "desc": "Template to render a Pulsar message.",
      "label": "Pulsar Message Template"
    },
    "pulsar_topic": {
      "desc": "Pulsar topic name",
      "label": "Pulsar Topic Name"
    },
    "retention_period": {
      "desc": "Specify the duration to buffer messages when disconnected from the Pulsar broker. Longer times increase memory/disk usage.",
      "label": "Retention Period"
    },
    "send_buffer": {
      "desc": "Fine tune the socket send buffer. The default value is tuned for high throughput.",
      "label": "Socket Send Buffer Size"
    },
    "strategy": {
      "desc": "Choose how messages are dispatched to Pulsar partitions:<br/><code>random</code>: Messages are randomly assigned to partitions.<br/><code>roundrobin</code>: Messages are evenly distributed across available producers.<br/><code>key_dispatch</code>: Partitions to be selected are hashed and stored in the Pulsar message key of the first message in a batch.",
      "label": "Partition Strategy"
    },
    "sync_timeout": {
      "desc": "Maximum wait time for receipt in synchronously publishing.",
      "label": "Sync Publish Timeout"
    },
    "message_value": {
      "desc": "Template to render Pulsar message value.",
      "label": "Message Value"
    },
    "pulsar_producer_struct": {
      "desc": "Configuration for a Pulsar bridge.",
      "label": "Pulsar Bridge Configuration"
    },
    "servers": {
      "desc": "Specify the Pulsar Server URL in the format of <code>scheme://host[:port]</code> for the client to connect to. Multiple servers should be added as a comma-separated list. The supported schemes are <code>pulsar://</code> (default) and <code>pulsar+ssl://</code>. The default port is 6650.",
      "label": "Servers"
    },
    "connect_timeout": {
      "desc": "Maximum wait time for TCP connection establishment (including authentication time if enabled).",
      "label": "Connect Timeout"
    }
  },
  "emqx_ee_bridge_azure_event_hub": {
    "bootstrap_hosts": {
      "desc": "A comma separated list of Azure Event Hub Kafka <code>host[:port]</code> namespace endpoints to bootstrap the client.  Default port number is 9093.",
      "label": "Bootstrap Hosts"
    },
    "connect_timeout": {
      "desc": "Maximum wait time for TCP connection establishment (including authentication time if enabled).",
      "label": "Connect Timeout"
    },
    "min_metadata_refresh_interval": {
      "desc": "Minimum time interval the client has to wait before refreshing Azure Event Hub Kafka broker and topic metadata. Setting too small value may add extra load on Azure Event Hub.",
      "label": "Min Metadata Refresh Interval"
    },
    "metadata_request_timeout": {
      "desc": "Maximum wait time when fetching metadata from Azure Event Hub.",
      "label": "Metadata Request Timeout"
    },
    "password": {
      "desc": "The password for connecting to Azure Event Hub.  Should be the \"connection string-primary key\" of a Namespace shared access policy.",
      "label": "Password"
    },
    "sndbuf": {
      "desc": "Fine tune the socket send buffer. The default value is tuned for high throughput.",
      "label": "Socket Send Buffer Size"
    },
    "recbuf": {
      "desc": "Fine tune the socket receive buffer. The default value is tuned for high throughput.",
      "label": "Socket Receive Buffer Size"
    },
    "tcp_keepalive": {
      "desc": "Enable TCP keepalive for Azure Event Hub bridge connections.\nThe value is three comma separated numbers in the format of 'Idle,Interval,Probes'\n - Idle: The number of seconds a connection needs to be idle before the server begins to send out keep-alive probes (Linux default 7200).\n - Interval: The number of seconds between TCP keep-alive probes (Linux default 75).\n - Probes: The maximum number of TCP keep-alive probes to send before giving up and killing the connection if no response is obtained from the other end (Linux default 9).\nFor example \"240,30,5\" means: TCP keepalive probes are sent after the connection is idle for 240 seconds, and the probes are sent every 30 seconds until a response is received, if it misses 5 consecutive responses, the connection should be closed.\nDefault: 'none'",
      "label": "TCP keepalive options"
    },
    "topic": {
      "desc": "Event Hub name",
      "label": "Event Hub Name"
    },
    "max_batch_bytes": {
      "desc": "Maximum bytes to collect in an Azure Event Hub message batch. Most of the Kafka brokers default to a limit of 1 MB batch size. EMQX's default value is less than 1 MB in order to compensate Kafka message encoding overheads (especially when each individual message is very small). When a single message is over the limit, it is still sent (as a single element batch).",
      "label": "Max Batch Bytes"
    },
    "partition_strategy": {
      "desc": "Partition strategy is to tell the producer how to dispatch messages to Azure Event Hub partitions.\n\n<code>random</code>: Randomly pick a partition for each message\n<code>key_dispatch</code>: Hash Azure Event Hub message key to a partition number",
      "label": "Partition Strategy"
    },
    "required_acks": {
      "desc": "Required acknowledgements for Azure Event Hub partition leader to wait for its followers before it sends back the acknowledgement to EMQX Azure Event Hub producer\n\n<code>all_isr</code>: Require all in-sync replicas to acknowledge.\n<code>leader_only</code>: Require only the partition-leader's acknowledgement.",
      "label": "Required Acks"
    },
    "kafka_headers": {
      "desc": "Please provide a placeholder to be used as Azure Event Hub Headers<br/>\ne.g. <code>${'{'}pub_props{'}'}</code><br/>\nNotice that the value of the placeholder must either be an object:\n<code>{'{'}\\\"foo\\\": \\\"bar\\\"{'}'}</code>\nor an array of key-value pairs:\n<code>[{'{'}\\\"key\\\": \\\"foo\\\", \\\"value\\\": \\\"bar\\\"{'}'}]</code>",
      "label": "Azure Event Hub Headers"
    },
    "kafka_ext_headers": {
      "desc": "Please provide more key-value pairs for Azure Event Hub headers<br/>The key-value pairs here will be combined with the value of <code>kafka_headers</code> field before sending to Azure Event Hub.",
      "label": "Extra Azure Event Hub headers"
    },
    "kafka_header_value_encode_mode": {
      "desc": "Azure Event Hub headers value encode mode<br/>\n - NONE: only add binary values to Azure Event Hub headers;<br/>\n - JSON: only add JSON values to Azure Event Hub headers,\nand encode it to JSON strings before sending.",
      "label": "Azure Event Hub headers value encode mode"
    },
    "partition_count_refresh_interval": {
      "desc": "The time interval for Azure Event Hub producer to discover increased number of partitions.\nAfter the number of partitions is increased in Azure Event Hub, EMQX will start taking the\ndiscovered partitions into account when dispatching messages per <code>partition_strategy</code>.",
      "label": "Partition Count Refresh Interval"
    },
    "max_inflight": {
      "desc": "Maximum number of batches allowed for Azure Event Hub producer (per-partition) to send before receiving acknowledgement from Azure Event Hub. Greater value typically means better throughput. However, there can be a risk of message reordering when this value is greater than 1.",
      "label": "Max Inflight"
    },
    "query_mode": {
      "desc": "Query mode. Optional 'sync/async', default 'async'.",
      "label": "Query mode"
    },
    "sync_query_timeout": {
      "desc": "This parameter defines the timeout limit for synchronous queries. It applies only when the bridge query mode is configured to 'sync'.",
      "label": "Synchronous Query Timeout"
    },
    "key": {
      "desc": "Template to render Azure Event Hub message key. If the template is rendered into a NULL value (i.e. there is no such data field in Rule Engine context) then Azure Event Hub's <code>NULL</code> (but not empty string) is used.",
      "label": "Message Key"
    },
    "value": {
      "desc": "Template to render Azure Event Hub message value. If the template is rendered into a NULL value (i.e. there is no such data field in Rule Engine context) then Azure Event Hub's <code>NULL</code> (but not empty string) is used.",
      "label": "Message Value"
    },
    "timestamp": {
      "desc": "Which timestamp to use. The timestamp is expected to be a millisecond precision Unix epoch which can be in string format, e.g. <code>1661326462115</code> or <code>'1661326462115'</code>. When the desired data field for this template is not found, or if the found data is not a valid integer, the current system timestamp will be used.",
      "label": "Message Timestamp"
    },
    "kafka_ext_header_key": {
      "desc": "Key of the Azure Event Hub header. Placeholders in format of ${'{'}var{'}'} are supported.",
      "label": "Key"
    },
    "kafka_ext_header_value": {
      "desc": "Value of the Azure Event Hub header. Placeholders in format of ${'{'}var{'}'} are supported.",
      "label": "Value"
    },
    "mode": {
      "desc": "Message buffer mode.\n\n<code>memory</code>: Buffer all messages in memory. The messages will be lost in case of EMQX node restart\n<code>disk</code>: Buffer all messages on disk. The messages on disk are able to survive EMQX node restart.\n<code>hybrid</code>: Buffer message in memory first, when up to certain limit (see <code>segment_bytes</code> config for more information), then start offloading messages to disk, Like <code>memory</code> mode, the messages will be lost in case of EMQX node restart.",
      "label": "Buffer Mode"
    },
    "per_partition_limit": {
      "desc": "Number of bytes allowed to buffer for each Azure Event Hub partition. When this limit is exceeded, old messages will be dropped in a trade for credits for new messages to be buffered.",
      "label": "Per-partition Buffer Limit"
    },
    "segment_bytes": {
      "desc": "Applicable when buffer mode is set to <code>disk</code> or <code>hybrid</code>.\nThis value is to specify the size of each on-disk buffer file.",
      "label": "Segment File Bytes"
    },
    "memory_overload_protection": {
      "desc": "Applicable when buffer mode is set to <code>memory</code>\nEMQX will drop old buffered messages under high memory pressure. The high memory threshold is defined in config <code>sysmon.os.sysmem_high_watermark</code>. NOTE: This config only works on Linux.",
      "label": "Memory Overload Protection"
    }
  },
  "emqx_ee_bridge_kinesis": {
    "pool_size": {
      "desc": "The pool size.",
      "label": "Pool Size"
    },
    "payload_template": {
      "desc": "The template for formatting the outgoing messages.  If undefined, will send all the available context in JSON format.",
      "label": "Payload template"
    },
    "aws_access_key_id": {
      "desc": "Access Key ID for connecting to Amazon Kinesis.",
      "label": "AWS Access Key ID"
    },
    "aws_secret_access_key": {
      "desc": "AWS Secret Access Key for connecting to Amazon Kinesis.",
      "label": "AWS Secret Access Key"
    },
    "endpoint": {
      "desc": "The url of Amazon Kinesis endpoint.",
      "label": "Amazon Kinesis Endpoint"
    },
    "stream_name": {
      "desc": "The Amazon Kinesis Stream to publish messages to.",
      "label": "Amazon Kinesis Stream"
    },
    "partition_key": {
      "desc": "The Amazon Kinesis Partition Key associated to published message. Placeholders in format of ${'{'}var{'}'} are supported.",
      "label": "Partition key"
    },
    "max_retries": {
      "desc": "Max retry times if an error occurs when sending a request.",
      "label": "Max Retries"
    }
  },
  "emqx_ee_bridge_greptimedb": {
    "write_syntax": {
      "desc": "Conf of GreptimeDB gRPC protocol to write data points. Write syntax is a text-based format that provides the measurement, tag set, field set, and timestamp of a data point, and placeholder supported, which is the same as InfluxDB line protocol.\nSee also [InfluxDB 2.3 Line Protocol](https://docs.influxdata.com/influxdb/v2.3/reference/syntax/line-protocol/) and\n[GreptimeDB 1.8 Line Protocol](https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_tutorial/) </br>\nTLDR:</br>\n```\n<measurement>[,<tag_key>=<tag_value>[,<tag_key>=<tag_value>]] <field_key>=<field_value>[,<field_key>=<field_value>] [<timestamp>]\n```\nPlease note that a placeholder for an integer value must be annotated with a suffix `i`. For example `${'{'}payload.int_value{'}'}i`.",
      "label": "Write Syntax"
    },
    "server": {
      "desc": "The IPv4 or IPv6 address or the hostname to connect to.</br>\nA host entry has the following form: `Host[:Port]`.</br>\nThe GreptimeDB default port 8086 is used if `[:Port]` is not specified.",
      "label": "Server Host"
    },
    "dbname": {
      "desc": "GreptimeDB database.",
      "label": "Database"
    },
    "greptimedb": {
      "desc": "GreptimeDB's protocol. Support GreptimeDB v1.8 and before.",
      "label": "HTTP API Protocol"
    },
    "username": {
      "desc": "GreptimeDB username.",
      "label": "Username"
    },
    "password": {
      "desc": "GreptimeDB password.",
      "label": "Password"
    },
    "precision": {
      "desc": "GreptimeDB time precision.",
      "label": "Time Precision"
    },
    "protocol": {
      "desc": "GreptimeDB's protocol. gRPC API.",
      "label": "Protocol"
    }
  }
}